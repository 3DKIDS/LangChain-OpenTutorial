{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Example: Prompt+Model+OutputParser\n",
    "\n",
    "- Author: [ChangJun Lee](https://www.linkedin.com/in/cjleeno1/)\n",
    "- Design: []()\n",
    "- Peer Review: []()\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab] [![Open in LangChain Academy]\n",
    "\n",
    "## Overview\n",
    "\n",
    "The most fundamental and commonly used case involves linking a prompt template with a model. To illustrate how this works, let us create a chain that asks for the capital cities of various countries.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Implementing the Comma-Separated List Output Parser](#implementing-the-comma-separated-list-output-parser)\n",
    "- [Using Streamed Outputs](#using-streamed-outputs)\n",
    "\n",
    "### References\n",
    "\n",
    "- [LangChain ChatOpenAI API reference](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n",
    "- [LangChain Core Output Parsers](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.list.CommaSeparatedListOutputParser.html#)\n",
    "- [Python List Tutorial](https://docs.python.org/3.13/tutorial/datastructures.html)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_openai\",\n",
    "        \"langchain_community\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"02-CommaSeparatedListOutputParser\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "You can alternatively set `OPENAI_API_KEY` in `.env` file and load it. \n",
    "\n",
    "[Note] This is not necessary if you've already set `OPENAI_API_KEY` in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration File for Managing API Key as an Environment Variable\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API KEY Information\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LangSmith tracking at [https://smith.langchain.com](https://smith.langchain.com).\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# Enter the project name.\n",
    "logging.langsmith(\"CH01-Basic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing Prompt Templates\n",
    "\n",
    "`PromptTemplate`\n",
    "\n",
    "- A prompt template is used to create a complete prompt string by incorporating the user's input variables.\n",
    "- Usage\n",
    "  - `template`: A template string is a predefined format where curly braces '{}' are used to represent variables.\n",
    "\n",
    "  - `input_variables`: The names of the variables to be inserted within the curly braces are defined as a list.\n",
    "\n",
    "`input_variables`\n",
    "\n",
    "- `input_variables` is a list that defines the names of the variables used in the `PromptTemplate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_response  # Streaming Output\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `from_template()` method is used to create a `PromptTemplate` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define template\n",
    "template = \"What is the capital of {country}?\"\n",
    "\n",
    "# Create a `PromptTemplate` object using the `from_template` method.\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the prompt.\n",
    "prompt = prompt_template.format(country=\"Korea\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the prompt.\n",
    "prompt = prompt_template.format(country=\"USA\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Creation\n",
    "\n",
    "### LCEL (LangChain Expression Language)\n",
    "\n",
    "Here, we use LCEL to combine various components into a single chain.\n",
    "\n",
    "![lcel.png](./images/lcel.png)\n",
    "\n",
    "```\n",
    "chain = prompt | model | output_parser\n",
    "```\n",
    "\n",
    "The `|` symbol works similarly to the [Unix pipe operator](<https://en.wikipedia.org/wiki/Pipeline_(Unix)>), linking different components and passing the output of one component as the input to the next.\n",
    "\n",
    "In this chain, user input is passed to the prompt template, and the output from the prompt template is then forwarded to the model. By examining each component individually, you can understand what happens at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt as a `PromptTemplate` object.\n",
    "prompt = PromptTemplate.from_template(\"Please explain {topic} in simple terms.\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking `invoke()`\n",
    "\n",
    "- Input values are provided in the form of a Python dictionary (key-value pairs).  \n",
    "- When calling the `invoke()` function, these input values are passed as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the topic in the `input` dictionary to 'The Principles of Learning in Artificial Intelligence Models'.\n",
    "input = {\"topic\": \"The Principles of Learning in Artificial Intelligence Models\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect the `prompt` object and the `model` object using the pipe (`|`) operator.  \n",
    "# Use the `invoke` method to pass the `input`.  \n",
    "# This will return the message generated by the AI model.\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of outputting a streaming response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request for Streaming Output\n",
    "answer = chain.stream(input)\n",
    "\n",
    "# Streaming Output\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An output parser is added to the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A processing chain is constructed by connecting the prompt, model, and output parser.\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `invoke` method of the `chain` object is used to pass the input.\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request for Streaming Output\n",
    "answer = chain.stream(input)\n",
    "\n",
    "# Streaming Output\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying and Modifying Templates\n",
    "\n",
    "- The prompt content below can be **modified** as needed for testing purposes.  \n",
    "- The `model_name` can also be adjusted for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a seasoned English teacher with 10 years of experience. Please write an English conversation suitable for the given situation.  \n",
    "Refer to the [FORMAT] for the structure.\n",
    "\n",
    "#SITUATION:\n",
    "{question}\n",
    "\n",
    "#FORMAT:\n",
    "- English Conversation:\n",
    "- Translation(in Korean):\n",
    "\"\"\"\n",
    "\n",
    "# A prompt is generated using the prompt template.\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Initialize the ChatOpenAI chat model.\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "# Initialize the string output parser.\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the chain.\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the completed Chain to obtain a response.\n",
    "print(chain.invoke({\"question\": \"I want to go to a restaurant and order food.\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the completed Chain to obtain a response  \n",
    "# Request for Streaming Output\n",
    "answer = chain.stream({\"question\": \"ㅑ would like to go to a restaurant and order some food.\"})\n",
    "\n",
    "# Streaming Output\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English Conversation:\n",
    "- Hello, could I see the menu, please? \n",
    "- I'd like to order the grilled salmon and a side of mashed potatoes.\n",
    "- Could I have a glass of water as well?\n",
    "- Thank you!\n",
    "\n",
    "Translation(in Korean):\n",
    "- 안녕하세요, 메뉴판 좀 볼 수 있을까요?\n",
    "- 구운 연어와 매시드 포테이토를 주문하고 싶어요.\n",
    "- 물 한 잔도 주실 수 있나요?\n",
    "- 감사합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time, set the question to 'Ordering Pizza in the US' and execute it.  \n",
    "# Request for Streaming Output\n",
    "answer = chain.stream({\"question\": \"Ordering Pizza in the US\"})\n",
    "\n",
    "# Streaming Output\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English Conversation:\n",
    "- Employee: \"Hello, Tony's Pizza. How can I help you?\"\n",
    "- Customer: \"Hi, I'd like to place an order for delivery, please.\"\n",
    "- Employee: \"Sure thing! What would you like to order?\"\n",
    "- Customer: \"I'll have a large pepperoni pizza with extra cheese and a side of garlic bread.\"\n",
    "- Employee: \"Anything to drink?\"\n",
    "- Customer: \"Yes, a 2-liter bottle of Coke, please.\"\n",
    "- Employee: \"Alright, your total comes to $22.50. Can I have your delivery address?\"\n",
    "- Customer: \"It's 742 Evergreen Terrace.\"\n",
    "- Employee: \"Thank you. Your order will be there in about 30-45 minutes. Is there anything else I can help you with?\"\n",
    "- Customer: \"No, that's everything. Thank you!\"\n",
    "- Employee: \"Thank you for choosing Tony's Pizza. Have a great day!\"\n",
    "\n",
    "Translation(in Korean):\n",
    "- 직원: \"안녕하세요, 토니의 피자입니다. 어떻게 도와드릴까요?\"\n",
    "- 고객: \"안녕하세요, 배달 주문하고 싶은데요.\"\n",
    "- 직원: \"네, 무엇을 주문하시겠어요?\"\n",
    "- 고객: \"큰 사이즈의 페퍼로니 피자에 치즈 추가하고, 마늘빵 하나 주세요.\"\n",
    "- 직원: \"음료는 드릴까요?\"\n",
    "- 고객: \"네, 콜라 2리터 한 병 주세요.\"\n",
    "- 직원: \"알겠습니다, 합계는 $22.50입니다. 배달 주소를 알려주시겠어요?\"\n",
    "- 고객: \"742 에버그린 테라스입니다.\"\n",
    "- 직원: \"감사합니다. 주문하신 음식은 대략 30-45분 내에 도착할 예정입니다. 다른 도움이 필요하신가요?\"\n",
    "- 고객: \"아니요, 이게 다예요. 감사합니다!\"\n",
    "- 직원: \"토니의 피자를 선택해주셔서 감사합니다. 좋은 하루 되세요!\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
