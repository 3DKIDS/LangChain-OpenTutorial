{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Expression Language(LCEL)\n",
    "\n",
    "- Author: [Erika](https://github.com/ErikaPark)\n",
    "- Peer Review: [Wooseok Jeong](https://github.com/jeong-wooseok), [JeongGi Park](https://github.com/jeongkpa)\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/sub-graph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239937-lesson-2-sub-graphs)\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "**LangChain Expression Language (LCEL)** is a declarative expression language used within the LangChain framework to define and customize the behavior of chains and agents. It is designed as a flexible tool that enables users to simplify complex workflows and make better use of LangChain's capabilities in an intuitive manner.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "\n",
    "### References\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "!pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_core\",\n",
    "        \"langchain-anthropic\",\n",
    "        \"langchain_community\",\n",
    "        \"langchain_text_splitters\",\n",
    "        \"langchain_openai\",\n",
    "        \"langchain-teddynote\"\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"LCEL\",  \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration file to manage the API KEY as an environment variable\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API KEY information\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Basic Example: Prompt + Model + Output Parser\n",
    "The most fundamental and common use case is connecting a prompt template with a model. To demonstrate how this works, let's create a Chain that asks for the capital cities of various countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH01-Basic\n"
     ]
    }
   ],
   "source": [
    "# Set up LangSmith tracking: https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# Enter the project name\n",
    "logging.langsmith(\"CH01-Basic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing Prompt Templates\n",
    "\n",
    "`PromptTemplate`\n",
    "\n",
    "- A prompt template is a structured tool used to dynamically generate prompts based on user input data.\n",
    "- Usage\n",
    "  - `template`: A template string where curly braces `{}` represent variables.\n",
    "  - `input_variables`: A list defining the names of variables to be substituted into the curly braces.\n",
    "\n",
    "`input_variables`\n",
    "\n",
    "- `input_variables` is a list that defines the names of the variables used in the `PromptTemplate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_response  # Streaming Output\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `PromptTemplate` object using the `from_template()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='What is the capital of {country}?')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the template\n",
    "template = \"What is the capital of {country}?\"\n",
    "\n",
    "# Create a PromptTemplate object using the from_template method\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of South Korea?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the prompt\n",
    "prompt = prompt_template.format(country=\"South Korea\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of United States?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the prompt\n",
    "prompt = prompt_template.format(country=\"United States\")\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_tokens=2048,\n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Chain\n",
    "\n",
    "### LCEL(LangChain Expression Language)\n",
    "\n",
    "![lcel.png](./images/lcel.png)\n",
    "\n",
    "Here, we use LCEL to combine various components into a single chain:\n",
    "\n",
    "```\n",
    "chain = prompt | model | output_parser\n",
    "```\n",
    "\n",
    "The `|`` operator, similar to the [Unix pipe operator](<https://en.wikipedia.org/wiki/Pipeline_(Unix)>)connects different components, passing the output of one component as the input to the next.\n",
    "\n",
    "In this chain:\n",
    "\n",
    "1. User input is passed to the prompt template.\n",
    "2. The prompt template's output is then sent to the model.\n",
    "3. Each component can be examined individually to understand what is happening at each stage.\n",
    "\n",
    "This modular design simplifies the process of connecting inputs, processing, and outputs into a streamlined workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt as a PromptTemplate object\n",
    "prompt = PromptTemplate.from_template(\"Please explain {topic} in simple terms.\")\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Combine the prompt and model into a chain\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling `invoke()`\n",
    "* Input values are passed as a Python dictionary (key: value format).\n",
    "* The `invoke()` function is used to call the chain and process the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input dictionary with the topic 'How AI models learn'\n",
    "input = {\"topic\": \"How AI models learn\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please explain How AI models learn in simple terms.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt.format(**input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='AI models learn by being trained on large amounts of data. The data is fed into the model, which then uses algorithms to analyze and identify patterns and relationships within the data. As the model processes more data, it adjusts and refines its algorithms to improve its accuracy and performance. This process is known as machine learning, where the model learns from the data it is exposed to and becomes more proficient at making predictions or decisions based on that data. Over time, the model becomes more accurate and reliable as it continues to learn from new data and experiences.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 111, 'prompt_tokens': 17, 'total_tokens': 128, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-36803344-0ff4-4403-94d1-7ae91b301bda-0', usage_metadata={'input_tokens': 17, 'output_tokens': 111, 'total_tokens': 128, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect the prompt object and model object using the pipe (|) operator\n",
    "# Use the invoke method to pass the input\n",
    "# This returns the message generated by the AI model\n",
    "response = chain.invoke(input)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of streaming output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI models learn through a process called machine learning, which involves feeding the model large amounts of data and allowing it to identify patterns and relationships within that data. \n",
      "\n",
      "The model uses algorithms to analyze the data and make predictions based on what it has learned. As the model is exposed to more data, it adjusts and refines its predictions, improving its accuracy over time.\n",
      "\n",
      "In simple terms, AI models learn by being exposed to data, identifying patterns, and refining their predictions based on that data. The more data they are exposed to, the better they become at making accurate predictions."
     ]
    }
   ],
   "source": [
    "# Request for streaming output\n",
    "answer = chain.stream(input)\n",
    "# Streaming output\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parser\n",
    "\n",
    "An **Output Parser** is a tool designed to transform or process the responses from an AI model into a specific format. Since the model's output is typically provided as free-form text, an **Output Parser** is essential to convert it into a structured format or extract the required data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser() # Directly returns the model's response as a string without modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add an output parser to the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the prompt, model, and output parser to create a processing chain\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='AI models learn by processing large amounts of data and adjusting their parameters to improve their performance in completing a specific task. The process can be broken down into the following steps:\\n\\n1. Data Collection: The AI model is trained on a large dataset that contains examples of input data and the corresponding correct output. This data is used to teach the model how to perform the task it is designed for.\\n\\n2. Training: The AI model processes the input data and makes predictions based on its current parameters. It compares these predictions to the correct output from the dataset and calculates how well it performed. The model then adjusts its parameters to minimize the error between its predictions and the correct output.\\n\\n3. Evaluation: The model is tested on a separate dataset to assess how well it has learned from the training data. This evaluation helps to determine if the model is performing well and if further adjustments are needed.\\n\\n4. Iteration: The training and evaluation process is repeated multiple times, with the model adjusting its parameters each time to improve its performance. This iterative process allows the AI model to learn from its mistakes and become more accurate in completing the task.\\n\\nIn simple terms, AI models learn by practicing on a large dataset, making predictions, comparing those predictions to the correct answers, and adjusting their parameters to get better at the task they are designed for.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 266, 'prompt_tokens': 17, 'total_tokens': 283, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e44e4132-6347-4e0f-a39d-87c5ee25b717-0', usage_metadata={'input_tokens': 17, 'output_tokens': 266, 'total_tokens': 283, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the invoke method of the chain object to pass the input\n",
    "input = {\"topic\": \"How AI models learn\"}\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI models learn by processing large amounts of data and using algorithms to identify patterns and relationships within that data. These models are trained on labeled data, which means that the data is already categorized or labeled with the correct answer. \n",
      "\n",
      "During the training process, the AI model adjusts its parameters and weights based on feedback it receives from the labeled data. This feedback helps the model to make more accurate predictions or classifications in the future. \n",
      "\n",
      "Through this iterative process of training and feedback, the AI model learns to recognize patterns and make predictions on new, unseen data. In essence, AI models learn by continuously refining their algorithms and improving their accuracy based on the data they are trained on."
     ]
    }
   ],
   "source": [
    "# Request for streaming output\n",
    "answer = chain.stream(input)\n",
    "# Streaming output\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Changes to the Template\n",
    "\n",
    "- You can **modify** the prompt content below and test it as needed.\n",
    "- You can also change the `model_name` for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a teacher with 10 years of experience teaching English. Please create an English conversation that fits the given situation.\n",
    "Refer to the [FORMAT] to structure your response.\n",
    "\n",
    "# Situation:\n",
    "{question}\n",
    "\n",
    "#FORMAT:\n",
    "- Dialogue in English:\n",
    "- Explanation of the Dialogue: \n",
    "\"\"\"\n",
    "\n",
    "# Generate the prompt using the PromptTemplate\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Initialize the ChatOpenAI model\n",
    "model = ChatOpenAI(model_name=\"gpt-4-turbo\")\n",
    "\n",
    "# Initialize the string output parser\n",
    "output_parser = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the chain\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Dialogue in English:\n",
      "A: \"Good evening. I’d like a table for two, please.\"\n",
      "\n",
      "Host: \"Certainly! Do you have a reservation?\"\n",
      "\n",
      "A: \"No, we don’t. Is there a wait?\"\n",
      "\n",
      "Host: \"Just a short one. It should be about 10 minutes. May I have your name, please?\"\n",
      "\n",
      "A: \"It’s Alex.\"\n",
      "\n",
      "Host: \"Thank you, Alex. We’ll call you as soon as your table is ready.\"\n",
      "\n",
      "[10 minutes later]\n",
      "\n",
      "Host: \"Alex, your table is ready. Please follow me.\"\n",
      "\n",
      "A: \"Thank you.\"\n",
      "\n",
      "[Seated at the table]\n",
      "\n",
      "Waiter: \"Hello, I’m Jim and I’ll be your server today. Can I start you off with something to drink?\"\n",
      "\n",
      "A: \"Yes, could we have two glasses of water, please? And also, could you bring us the wine list?\"\n",
      "\n",
      "Waiter: \"Absolutely, I’ll be right back with your waters and the wine list.\"\n",
      "\n",
      "[Waiter returns with drinks and the wine list]\n",
      "\n",
      "Waiter: \"Here you are. Are you ready to order, or do you need a few more minutes?\"\n",
      "\n",
      "A: \"I think we’ll need a few more minutes. We’re still deciding.\"\n",
      "\n",
      "Waiter: \"No problem at all. I’ll come back in a few minutes. Please take your time.\"\n",
      "\n",
      "[After a while]\n",
      "\n",
      "Waiter: \"Have you decided on what you’d like to eat tonight?\"\n",
      "\n",
      "A: \"Yes, I’ll have the steak medium-rare, and for the side, I’ll go with the mashed potatoes.\"\n",
      "\n",
      "Waiter: \"Great choice! And for you?\"\n",
      "\n",
      "B: \"I’ll have the grilled salmon with a side salad, please.\"\n",
      "\n",
      "Waiter: \"Excellent. I’ll place your orders right away. Is there anything else I can get you in the meantime?\"\n",
      "\n",
      "A: \"No, that’s all for now, thank you.\"\n",
      "\n",
      "Waiter: \"Very well, I’ll be back with your dishes as soon as they’re ready. Enjoy your meal!\"\n",
      "\n",
      "# Explanation:\n",
      "This conversation is a typical example of dining out at a restaurant. It begins with arriving and requesting a table, followed by interaction with the host and being seated. The waiter then takes over, offering drinks and taking meal orders. The language used is polite and formal, suitable for a dining context. Key phrases like \"I’d like,\" \"May I have,\" \"Are you ready to order,\" and \"I’ll have\" are commonly used in restaurant settings. The dialogue not only reflects the steps of ordering food but also demonstrates courteous exchanges between the customers and restaurant staff.\n"
     ]
    }
   ],
   "source": [
    "# Execute the completed chain to get the response\n",
    "print(chain.invoke({\"question\": \"I want to go to a restaurant and order food\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Dialogue in English:\n",
      "**Alex:** Good evening! Could I see the menu, please?\n",
      "\n",
      "**Waiter:** Of course! Here you go. Our special today is grilled salmon with asparagus. Would you like to start with something to drink?\n",
      "\n",
      "**Alex:** Yes, I'll have a glass of white wine, please. Could you recommend a wine that goes well with salmon?\n",
      "\n",
      "**Waiter:** Certainly! The Chardonnay is excellent with our salmon dish. It’s light and complements the flavors beautifully.\n",
      "\n",
      "**Alex:** Great, I'll have a glass of Chardonnay then. And for the meal, I'll try the special, the grilled salmon.\n",
      "\n",
      "**Waiter:** Excellent choice! Would you like any sides or a salad with that?\n",
      "\n",
      "**Alex:** What side dishes do you have?\n",
      "\n",
      "**Waiter:** We have mashed potatoes, steamed vegetables, or a house salad.\n",
      "\n",
      "**Alex:** I'll go with steamed vegetables, please.\n",
      "\n",
      "**Waiter:** Perfect. Your order will be ready shortly. Enjoy your meal!\n",
      "\n",
      "**Alex:** Thank you!\n",
      "\n",
      "# Explanation:\n",
      "This conversation is a typical interaction between a customer (Alex) and a waiter at a restaurant. The dialogue illustrates how to request a menu, inquire about daily specials, and ask for recommendations, which are common aspects of dining out. Alex also makes decisions based on the waiter's suggestions, showing an example of how to order food and drinks politely in an English-speaking restaurant context. The language used is polite and appropriate for a dining setting, incorporating phrases like \"Could I see,\" \"Would you like,\" and \"I'll have,\" which are standard in restaurant conversations. The waiter provides helpful and prompt responses, enhancing the service experience."
     ]
    }
   ],
   "source": [
    "# Execute the completed chain to get the response\n",
    "# Request for streaming output\n",
    "answer = chain.stream({\"question\": \"I want to go to a restaurant and order food\"})\n",
    "# Streaming output\n",
    "stream_response(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Dialogue in English:\n",
      "**Customer:** Hi, I’d like to order a pizza for delivery, please.\n",
      "\n",
      "**Pizza Shop Employee:** Sure, what would you like to order today?\n",
      "\n",
      "**Customer:** I’d like a large pepperoni pizza with extra cheese.\n",
      "\n",
      "**Pizza Shop Employee:** Would you like to add any sides or drinks to your order?\n",
      "\n",
      "**Customer:** Yes, can I get an order of garlic bread and two cans of Coke?\n",
      "\n",
      "**Pizza Shop Employee:** Absolutely. Can I have your delivery address, please?\n",
      "\n",
      "**Customer:** It's 452 Maple Street, Apartment 2B.\n",
      "\n",
      "**Pizza Shop Employee:** Thank you. And what's the best phone number to reach you?\n",
      "\n",
      "**Customer:** My number is 555-123-4567.\n",
      "\n",
      "**Pizza Shop Employee:** Great, your total comes to $24.50. It should take about 30-45 minutes for your delivery. Is there anything else I can help you with?\n",
      "\n",
      "**Customer:** No, that’s all. Thank you!\n",
      "\n",
      "**Pizza Shop Employee:** You’re welcome! Enjoy your pizza and have a great day!\n",
      "\n",
      "# Explanation:\n",
      "This dialogue exemplifies a typical phone conversation for ordering pizza in the United States. The customer specifies the type of pizza and additional items, while the employee confirms the order details such as the delivery address and contact number. The conversation includes polite exchanges, confirmation of order details, and an estimated delivery time, all of which are common elements in service-oriented phone calls in the U.S."
     ]
    }
   ],
   "source": [
    "# This time, set the question to 'Ordering pizza in the United States' and execute it.\n",
    "# Request for streaming output\n",
    "answer = chain.stream({\"question\": \"Ordering pizza in the United States\"})\n",
    "# Streaming output\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-i-KKkGhc-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
