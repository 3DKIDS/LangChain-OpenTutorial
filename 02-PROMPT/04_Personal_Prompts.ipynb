{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3FhQbSSEKAq"
      },
      "source": [
        "# Personal Prompts for LangChain\n",
        "\n",
        "- Author: [Eun](https://github.com/yuneun92)\n",
        "- Design: [Teddy](https://github.com/teddylee777)\n",
        "- Peer Review: [Minji-kang](https://github.com/r14minji), [Wooseok-Jeong](https://github.com/jeong-wooseok)\n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/sub-graph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239937-lesson-2-sub-graphs)\n",
        "\n",
        "#  Overview\n",
        "\n",
        "This cookbook contains a comprehensive collection of specialized prompts designed for various professional domains using LangChain. The prompts are crafted to leverage the power of large language models while maintaining domain expertise and professional standards.\n",
        "\n",
        "> You will learn how to create prompts for the following:\n",
        "- **Input Preparation**\n",
        "   - Verify all required parameters\n",
        "   - Format input data correctly\n",
        "   - Validate data completeness\n",
        "   - Check for data consistency\n",
        "- **Output Processing**\n",
        "   - Validate JSON structure\n",
        "   - Check for required fields\n",
        "   - Process nested data appropriately\n",
        "   - Handle error cases gracefully\n",
        "- **Quality Control**\n",
        "   - Review outputs for accuracy\n",
        "   - Validate domain-specific requirements\n",
        "   - Ensure compliance with standards\n",
        "   - Monitor performance metrics\n",
        "   \n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Prompt Generating Tips](#prompt-generating-tips)\n",
        "- [Basic Prompts](#basic-prompts)\n",
        "- [Advanced Prompts](#advanced-prompts)\n",
        "- [Specialized Prompts](#specialized-prompts)\n",
        "- [Professional Domain Prompts](#professional-domain-prompts)\n",
        "\n",
        "### References\n",
        "\n",
        "- [Prompt Engineering Guide: Gemini](https://www.promptingguide.ai/models/gemini)\n",
        "- [Google: Prompting Guide 101](https://services.google.com/fh/files/misc/gemini-for-google-workspace-prompting-guide-101.pdf)\n",
        "- [Anthropic: Prompt Engineering - Use XML tags to structure your prompts\n",
        "](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/use-xml-tags)\n",
        "- [Anthropic: Prompt engineering overview](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview)\n",
        "- [Anthropic: Anthropic's Prompt Engineering Interactive Tutorial](https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit?gid=1733615301#gid=1733615301)\n",
        "- [Github: prompt-eng-interactive-tutorial](https://github.com/anthropics/prompt-eng-interactive-tutorial)\n",
        "- [The Decoder: Chat GPT Guide](https://the-decoder.com/chatgpt-guide-prompt-strategies/)\n",
        "- [Dorik: How to Write Prompts for ChatGPT (with Examples)](https://dorik.com/blog/how-to-write-prompts-for-chatgpt)\n",
        "- [Coursera: How To Write ChatGPT Prompts: Your 2025 Guide](https://www.coursera.org/articles/how-to-write-chatgpt-prompts)\n",
        "- [LangSmith: Prompt Hub](https://docs.smith.langchain.com/old/hub/dev-setup)\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujWGc2x1KTAN"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
        "\n",
        "**[Note]**\n",
        "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials.\n",
        "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HG2L3xhaKSIw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × pip subprocess to install backend dependencies did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [29 lines of output]\n",
            "      Collecting distribute\n",
            "        Downloading distribute-0.7.3.zip (145 kB)\n",
            "        Installing build dependencies: started\n",
            "        Installing build dependencies: finished with status 'done'\n",
            "        Getting requirements to build wheel: started\n",
            "        Getting requirements to build wheel: finished with status 'done'\n",
            "        Preparing metadata (pyproject.toml): started\n",
            "        Preparing metadata (pyproject.toml): finished with status 'error'\n",
            "        error: subprocess-exited-with-error\n",
            "      \n",
            "        횞 Preparing metadata (pyproject.toml) did not run successfully.\n",
            "        \\xe2봻 exit code: 1\n",
            "        \\xe2빊\\xe2\\x94\\x80> [6 lines of output]\n",
            "            usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n",
            "               or: setup.py --help [cmd1 cmd2 ...]\n",
            "               or: setup.py --help-commands\n",
            "               or: setup.py cmd --help\n",
            "      \n",
            "            error: invalid command 'dist_info'\n",
            "            [end of output]\n",
            "      \n",
            "        note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "      error: metadata-generation-failed\n",
            "      \n",
            "      횞 Encountered error while generating package metadata.\n",
            "      \\xe2빊\\xe2\\x94\\x80> See above for output.\n",
            "      \n",
            "      note: This is an issue with the package mentioned above, not pip.\n",
            "      hint: See above for details.\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: subprocess-exited-with-error\n",
            "\n",
            "× pip subprocess to install backend dependencies did not run successfully.\n",
            "│ exit code: 1\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "%%capture --no-stderr\n",
        "!pip install dotenv langchain-opentutorial langchain langchainhub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "J_ILb4kLiJ3n"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langsmith\",\n",
        "        \"langchain\",\n",
        "        \"langchain_core\",\n",
        "        \"langchainhub\"\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(override=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZoxdCFP7iK5Z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment variables have been set successfully.\n"
          ]
        }
      ],
      "source": [
        "# Set environment variables\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "set_env(\n",
        "    {\n",
        "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "        \"LANGCHAIN_PROJECT\": \"Personal Prompts for LangChain\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo87C1reS4rC"
      },
      "source": [
        "## Prompt Generating Tips\n",
        "There are several LLM models which offer great performance these days. Each model has different charactor and fearures.     \n",
        "By following these tailored tips, you can maximize the strengths of each model and achieve optimal performance in your LangChain projects.\n",
        "\n",
        "### **Model Comparison at a Glance:**\n",
        "\n",
        "| Feature                | **ChatGPT**                                      | **Claude**                                      | **Gemini**                                      |\n",
        "|------------------------|--------------------------------------------------|------------------------------------------------|------------------------------------------------|\n",
        "| **Strengths**           | Conversational, logical reasoning               | Handles structured formats, logical responses  | Works well with detailed tasks and examples    |\n",
        "| **Best Practice**       | Clear, focused prompts                          | XML-style structured prompts                   | Detailed instructions and examples             |\n",
        "| **Example Use Case**    | Writing emails, casual conversations            | Analytical tasks, structured outputs           | Summaries, detailed reports, multimodal tasks  |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvQ5teCqwKvM"
      },
      "source": [
        "\n",
        "###  ChatGPT (OpenAI's GPT-4)**  \n",
        "ChatGPT is a powerful language model known for its conversational ability and logical reasoning.\n",
        "\n",
        "> Prompt Tips\n",
        ">- **Keep it Clear and Focused:**  Clearly define what you want the model to do. Don’t overload it with too much background information.\n",
        ">- **Ask for a Specific Format:**  If you need the response in bullet points, tables, or paragraphs, mention it.\n",
        ">- **Assign a Role:**  Tell ChatGPT who it is (e.g., \"You are a project manager\") to get more tailored answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bGRKNlD0Ezv-",
        "outputId": "8bd62e02-1547-4a00-b092-906b77c40d93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'You are a professional email writer. Write a polite email to a client informing them of a project delay of one month due to supply chain issues. The tone should be apologetic but confident.'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example Prompt for GPT4\n",
        "\"You are a professional email writer. Write a polite email to a client informing them of a project delay of one month due to supply chain issues. The tone should be apologetic but confident.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPoI2wEXEtZx"
      },
      "source": [
        "\n",
        "\n",
        "### Claude (Anthropic's Model)**  \n",
        "Claude excels in structured thinking and understanding detailed tasks. It often works well with **XML-style formatting** for prompts.\n",
        "\n",
        "> **Prompt Tips:**\n",
        ">- **Use Structured Formats:**  Use XML tags to organize the instructions, which helps Claude interpret them better.\n",
        ">- **Provide Context and Examples:**  Add a clear task and examples to guide the model's response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "X-J4xElqE8GH",
        "outputId": "bb199b26-fb57-4d21-c9c3-0d9ba7632bad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n<context>\\n  <project>\\n    <name>Website Redesign</name>\\n    <deadline>March 15, 2025</deadline>\\n  </project>\\n</context>\\n<instructions>\\n  Write an email to the client explaining the project will be delayed by one month due to supply chain issues. Apologize and propose a new deadline.\\n</instructions>\\n<example>\\n  Dear [Client Name],\\n\\n  Due to supply chain challenges, we regret to inform you that the project will be delayed. The new expected completion date is April 15, 2025. We apologize for the inconvenience and appreciate your understanding.\\n\\n  Best regards,\\n  [Your Name]\\n</example>\\n'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example Prompt for Claude\n",
        "\"\"\"\n",
        "<context>\n",
        "  <project>\n",
        "    <name>Website Redesign</name>\n",
        "    <deadline>March 15, 2025</deadline>\n",
        "  </project>\n",
        "</context>\n",
        "<instructions>\n",
        "  Write an email to the client explaining the project will be delayed by one month due to supply chain issues. Apologize and propose a new deadline.\n",
        "</instructions>\n",
        "<example>\n",
        "  Dear [Client Name],\n",
        "\n",
        "  Due to supply chain challenges, we regret to inform you that the project will be delayed. The new expected completion date is April 15, 2025. We apologize for the inconvenience and appreciate your understanding.\n",
        "\n",
        "  Best regards,\n",
        "  [Your Name]\n",
        "</example>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyExycWfExUW"
      },
      "source": [
        "### Gemini (Google’s AI Model)**  \n",
        "Gemini is a cutting-edge multimodal AI designed to work across text, images, and other data types. It handles detailed and structured tasks effectively.\n",
        "\n",
        "> **Prompt Tips:**\n",
        ">- **Be Detailed and Specific:**  Clearly explain the task and provide any necessary background details.\n",
        ">- **Break Complex Tasks into Steps:**  If the task is complicated, split it into smaller, sequential steps.\n",
        ">- **Add Examples:**  Providing examples helps Gemini align its output with your expectations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9MI5GzQcFFNt",
        "outputId": "9b945294-dea9-43f4-f31a-042d38b74e69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'You are a marketing strategist. Write a 200-word summary of the key milestones achieved in a project, emphasizing the team’s performance and results. Use a professional tone.'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Exmple Prompt for Gemini\n",
        "\"You are a marketing strategist. Write a 200-word summary of the key milestones achieved in a project, emphasizing the team’s performance and results. Use a professional tone.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFfeSRoCEj4I"
      },
      "source": [
        "## Basic Prompts\n",
        "Let's move onto Basic Prompts. This chapter covers summarization tasks that are most commonly used across all domains. <br>\n",
        "These prompts can be used individually or combined in a pipeline:\n",
        "\n",
        "\n",
        "- **Sequential Processing**\n",
        "   ```\n",
        "   documents → Summary Prompt → Map Prompt → Reduce Prompt → Final Output\n",
        "   ```\n",
        "\n",
        "- **Parallel Processing**\n",
        "   ```\n",
        "   documents → Multiple Summary Prompts (parallel)\n",
        "            → Map Prompts (parallel)\n",
        "            → Single Reduce Prompt\n",
        "            → Final Output\n",
        "   ```\n",
        "\n",
        "- **Hybrid Processing**\n",
        "   ```\n",
        "   documents → Summary Prompt\n",
        "            → Map Prompt (for themes)\n",
        "            → Reduce Prompt (for final synthesis)\n",
        "            → Additional Summary Prompt (for final polish)\n",
        "   ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary Prompt\n",
        "\n",
        "- The Summary Prompt is designed to create concise, informative summaries of documents while maintaining key information and context.\n",
        "    - Bullet-point format for clarity\n",
        "    - 💡Emoji integration for better engagement\n",
        "    - Focus on main points\n",
        "    - Elimination of redundant information\n",
        "    - Preservation of key context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "D5d4muPzSW06"
      },
      "outputs": [],
      "source": [
        "PROMPT_OWNER='eun'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jCgju8_E_V1",
        "outputId": "caac0a06-b18c-487e-b231-20c52b4e9044"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nPlease summarize the sentence according to the following REQUEST.\\nREQUEST:\\n1. Summarize the main points in bullet points.\\n2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.\\n3. Use various emojis to make the summary more interesting.\\n4. DO NOT include any unnecessary information.\\n\\nCONTEXT:\\n{context}\\n\\nSUMMARY:\"\\n')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain import hub\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Let's upload the prompt to the LangChain Hub.\n",
        "# Don't forget to enter the LangSmith API as an environment variable.\n",
        "prompt_title = \"summarize_document\"\n",
        "\n",
        "summarize_prompt = \"\"\"\n",
        "Please summarize the sentence according to the following REQUEST.\n",
        "REQUEST:\n",
        "1. Summarize the main points in bullet points.\n",
        "2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.\n",
        "3. Use various emojis to make the summary more interesting.\n",
        "4. DO NOT include any unnecessary information.\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "SUMMARY:\"\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(summarize_prompt)\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nW0KSsa-PImJ",
        "outputId": "d68ba0c0-dc69-408c-f778-6d8b67f18591"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Cannot create a public prompt without first\ncreating a LangChain Hub handle. You can add a handle by creating a public prompt at:\nhttps://smith.langchain.com/prompts",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# To upload a prompt to Hub:\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Private Repository:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# - Include your handle in the prompt title path\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# hub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt, new_repo_is_public=True)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPROMPT_OWNER\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprompt_title\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_repo_is_public\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\masta\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-wl7pahpy-py3.11\\Lib\\site-packages\\langchain\\hub.py:71\u001b[0m, in \u001b[0;36mpush\u001b[1;34m(repo_full_name, object, api_url, api_key, parent_commit_hash, new_repo_is_public, new_repo_description, readme, tags)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Then it's langsmith\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(client, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpush_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_full_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparent_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_public\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_repo_is_public\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_repo_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreadme\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreadme\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Then it's langchainhub\u001b[39;00m\n\u001b[0;32m     82\u001b[0m manifest_json \u001b[38;5;241m=\u001b[39m dumps(\u001b[38;5;28mobject\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\masta\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-wl7pahpy-py3.11\\Lib\\site-packages\\langsmith\\client.py:6549\u001b[0m, in \u001b[0;36mClient.push_prompt\u001b[1;34m(self, prompt_identifier, object, parent_commit_hash, is_public, description, readme, tags)\u001b[0m\n\u001b[0;32m   6545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt_exists(prompt_identifier):\n\u001b[0;32m   6546\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m   6547\u001b[0m         param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m [is_public, description, readme, tags]\n\u001b[0;32m   6548\u001b[0m     ):\n\u001b[1;32m-> 6549\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   6550\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprompt_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6551\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6552\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreadme\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreadme\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6553\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6554\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_public\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_public\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6555\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6556\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6557\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_prompt(\n\u001b[0;32m   6558\u001b[0m         prompt_identifier,\n\u001b[0;32m   6559\u001b[0m         is_public\u001b[38;5;241m=\u001b[39mis_public \u001b[38;5;28;01mif\u001b[39;00m is_public \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6562\u001b[0m         tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[0;32m   6563\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\masta\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-wl7pahpy-py3.11\\Lib\\site-packages\\langsmith\\client.py:6275\u001b[0m, in \u001b[0;36mClient.update_prompt\u001b[1;34m(self, prompt_identifier, description, readme, tags, is_public, is_archived)\u001b[0m\n\u001b[0;32m   6273\u001b[0m settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_settings()\n\u001b[0;32m   6274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_public \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mtenant_handle:\n\u001b[1;32m-> 6275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   6276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot create a public prompt without first\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6277\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreating a LangChain Hub handle. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can add a handle by creating a public prompt at:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://smith.langchain.com/prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6280\u001b[0m     )\n\u001b[0;32m   6282\u001b[0m json: Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, Sequence[\u001b[38;5;28mstr\u001b[39m]]] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   6284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m description \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mValueError\u001b[0m: Cannot create a public prompt without first\ncreating a LangChain Hub handle. You can add a handle by creating a public prompt at:\nhttps://smith.langchain.com/prompts"
          ]
        }
      ],
      "source": [
        "# To upload a prompt to Hub:\n",
        "#\n",
        "# Private Repository:\n",
        "# - Simply pass the prompt title as the first argument\n",
        "# hub.push(prompt_title, prompt, new_repo_is_public=False)\n",
        "#\n",
        "# Public Repository:\n",
        "# - First create a Hub Handle at LangSmith (smith.langchain.com)\n",
        "# - Include your handle in the prompt title path\n",
        "# hub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt, new_repo_is_public=True)\n",
        "\n",
        "hub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt, new_repo_is_public=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pkMouSCRXsA"
      },
      "source": [
        "You can find the uploaded prompt in your LangSmith. Please go to the site address as output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55Mcx-IBSwyK",
        "outputId": "86a2fc3a-5ae0-4223-ab76-6f189c598c12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'eun', 'lc_hub_repo': 'summarize_document', 'lc_hub_commit_hash': '129da0ee7cc02d076cd26692334f58a4aa898f5c40916847e8d808adb31f0263'}, template='\\nPlease summarize the sentence according to the following REQUEST.\\nREQUEST:\\n1. Summarize the main points in bullet points.\\n2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.\\n3. Use various emojis to make the summary more interesting.\\n4. DO NOT include any unnecessary information.\\n\\nCONTEXT:\\n{context}\\n\\nSUMMARY:\"\\n')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can import and use prompts as follows.\n",
        "prompt = hub.pull(\"eun/summarize_document:129da0ee\")\n",
        "prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj2GziG9F9TT"
      },
      "source": [
        "## 2. Map Prompt\n",
        "\n",
        "### Purpose\n",
        "The Map Prompt is used to extract and organize main themes from documents, creating a structured representation of the content.\n",
        "\n",
        "### Key Features\n",
        "- Structured theme extraction\n",
        "- Numbered list format\n",
        "- Comprehensive coverage\n",
        "- Concise presentation\n",
        "- Limited theme count for focus\n",
        "\n",
        "### Use Cases\n",
        "- Content analysis\n",
        "- Theme identification\n",
        "- Document classification\n",
        "- Information organization\n",
        "- Topic mapping\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIX0yuk0D7xZ",
        "outputId": "1a59fcf7-5e8f-49d2-8520-a786c037e62c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['docs'], input_types={}, partial_variables={}, template='\\nYou are a helpful expert journalist in extracting the main themes from a GIVEN DOCUMENTS below.\\nPlease provide a comprehensive summary of the GIVEN DOCUMENTS in numbered list format.\\nThe summary should cover all the key points and main ideas presented in the original text, while also condensing the information into a concise and easy-to-understand format.\\nPlease ensure that the summary includes relevant details and examples that support the main ideas, while avoiding any unnecessary information or repetition.\\nThe length of the summary should be appropriate for the length and complexity of the original text, providing a clear and accurate overview without omitting any important information.\\n\\nGIVEN DOCUMENTS:\\n{docs}\\n\\nFORMAT:\\n1. main theme 1\\n2. main theme 2\\n3. main theme 3\\n...\\n\\nCAUTION:\\n- DO NOT list more than 5 main themes.\\n\\nHelpful Answer:\\n')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain import hub\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_title = \"map-prompt\"\n",
        "\n",
        "map_prompt = \"\"\"\n",
        "You are a helpful expert journalist in extracting the main themes from a GIVEN DOCUMENTS below.\n",
        "Please provide a comprehensive summary of the GIVEN DOCUMENTS in numbered list format.\n",
        "The summary should cover all the key points and main ideas presented in the original text, while also condensing the information into a concise and easy-to-understand format.\n",
        "Please ensure that the summary includes relevant details and examples that support the main ideas, while avoiding any unnecessary information or repetition.\n",
        "The length of the summary should be appropriate for the length and complexity of the original text, providing a clear and accurate overview without omitting any important information.\n",
        "\n",
        "GIVEN DOCUMENTS:\n",
        "{docs}\n",
        "\n",
        "FORMAT:\n",
        "1. main theme 1\n",
        "2. main theme 2\n",
        "3. main theme 3\n",
        "...\n",
        "\n",
        "CAUTION:\n",
        "- DO NOT list more than 5 main themes.\n",
        "\n",
        "Helpful Answer:\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(map_prompt)\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "OQ3qVGX1GzrF",
        "outputId": "7cbc13c0-16f2-4b2e-f756-b10cf0e639c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://smith.langchain.com/prompts/map-prompt/1535fbd6?organizationId=d601689b-4a60-53f0-873e-4b704481b00c'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hub.push(prompt_title, prompt, new_repo_is_public=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxc8svsyGQfM"
      },
      "source": [
        "## 3. Reduce Prompt\n",
        "\n",
        "### Purpose\n",
        "The Reduce Prompt combines and synthesizes multiple summaries into a single, coherent output, particularly useful for processing large document sets.\n",
        "\n",
        "### Key Features\n",
        "- Multiple summary integration\n",
        "- Important insight extraction\n",
        "- Redundancy elimination\n",
        "- Coherent synthesis\n",
        "- Structured output format\n",
        "\n",
        "### Use Cases\n",
        "- Multi-document summarization\n",
        "- Information synthesis\n",
        "- Content consolidation\n",
        "- Research aggregation\n",
        "- Report generation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beKSYVn8D7xc",
        "outputId": "6f99f629-adcc-4422-d136-c0330b11dcac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['doc_summaries'], input_types={}, partial_variables={}, template='\\nYou are a helpful expert in summary writing.\\nYou are given numbered lists of summaries.\\nExtract top 10 most important insights and create a unified summary.\\n\\nLIST OF SUMMARIES:\\n{doc_summaries}\\n\\nREQUIREMENTS:\\n1. Identify key insights across summaries\\n2. Maintain coherence and flow\\n3. Eliminate redundancy\\n4. Preserve important details\\n5. Create a unified narrative\\n\\nOUTPUT FORMAT:\\n1. Main insights (bullet points)\\n2. Synthesized summary\\n3. Key takeaways\\n')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain import hub\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_title = \"reduce-prompt\"\n",
        "\n",
        "reduce_prompt = \"\"\"\n",
        "You are a helpful expert in summary writing.\n",
        "You are given numbered lists of summaries.\n",
        "Extract top 10 most important insights and create a unified summary.\n",
        "\n",
        "LIST OF SUMMARIES:\n",
        "{doc_summaries}\n",
        "\n",
        "REQUIREMENTS:\n",
        "1. Identify key insights across summaries\n",
        "2. Maintain coherence and flow\n",
        "3. Eliminate redundancy\n",
        "4. Preserve important details\n",
        "5. Create a unified narrative\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "1. Main insights (bullet points)\n",
        "2. Synthesized summary\n",
        "3. Key takeaways\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(reduce_prompt)\n",
        "prompt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AQEnto5xD7xc",
        "outputId": "bd9a657e-c5e0-42fc-f1ad-4c476b196c03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://smith.langchain.com/prompts/reduce-prompt/17ed176f?organizationId=d601689b-4a60-53f0-873e-4b704481b00c'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hub.push(prompt_title, prompt, new_repo_is_public=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdPw-wLbIq42"
      },
      "source": [
        "---\n",
        "# Advanced Prompts\n",
        "\n",
        "The Advanced Prompts chapter explores sophisticated techniques that enhance the quality and specificity of language model outputs. These prompts are designed to handle complex tasks requiring deeper analysis and more nuanced responses.\n",
        "\n",
        "## Key Components:\n",
        "\n",
        "> Chain of Density Summary\n",
        "\n",
        "Iteratively refines summaries for increasing information density\n",
        "Maintains consistent length while adding more information\n",
        "Focuses on key entities and relationships\n",
        "Produces progressively more concise yet informative outputs\n",
        "\n",
        "\n",
        "> Key Information Extraction\n",
        "\n",
        "Identifies and extracts critical data points\n",
        "Structures information in standardized formats\n",
        "Maintains contextual relationships\n",
        "Supports automated data processing\n",
        "\n",
        "\n",
        "> Metadata Tagging\n",
        "\n",
        "Automatically generates relevant tags and categories\n",
        "Enhances content searchability\n",
        "Improves content organization\n",
        "Facilitates information retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYBsvlfaIqz0"
      },
      "source": [
        "## 1. Chain of Density Summarization\n",
        "\n",
        "### Purpose\n",
        "> Chain of Density Summarization iteratively refines summaries to achieve higher information density while maintaining readability and key insights.\n",
        "\n",
        "### Key Features\n",
        "> - Progressive information density increase\n",
        "- Consistent length maintenance\n",
        "- Key entity focus\n",
        "- Contextual relationship preservation\n",
        "- Iterative refinement process\n",
        "\n",
        "### Use Cases\n",
        "> - Academic paper summarization\n",
        "- Technical documentation condensation\n",
        "- Research report synthesis\n",
        "- Complex document analysis\n",
        "- Information distillation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YAiX_AqKD7xc"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_title = \"chain-of-density\"\n",
        "\n",
        "chain_density_prompt = \"\"\"\n",
        "Given the input text, generate increasingly dense summaries through the following steps:\n",
        "\n",
        "INPUT PARAMETERS:\n",
        "- Text: {text}\n",
        "- Iteration Count: {iterations}\n",
        "- Target Length: {length}\n",
        "\n",
        "PROCESS:\n",
        "1. Initial Summary\n",
        "2. Entity Identification\n",
        "3. Density Enhancement\n",
        "4. Quality Check\n",
        "\n",
        "OUTPUT REQUIREMENTS:\n",
        "1. Maintain consistent length\n",
        "2. Increase information density\n",
        "3. Preserve key entities\n",
        "4. Ensure readability\n",
        "\n",
        "Please provide the summary following this structure:\n",
        "\n",
        "FORMAT:\n",
        "{\n",
        "    \"initial_summary\": str,\n",
        "    \"entity_map\": list,\n",
        "    \"refined_summaries\": list,\n",
        "    \"final_summary\": str\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(chain_density_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "wSVewIsdD7xd",
        "outputId": "bf8b4672-1c52-4883-9b20-696efa7a8454"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://smith.langchain.com/prompts/chain-of-density/a9eac13f?organizationId=f2bffb3c-dd45-53ac-b23b-5d696451d11c'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hub.push(prompt_title, prompt, new_repo_is_public=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RRmJ64fZpzz"
      },
      "source": [
        "### 1.1. Chain of Density (Multilingual)\n",
        "\n",
        "### Purpose\n",
        "> Generate increasingly dense summaries in any specified language through iterative refinement while maintaining semantic accuracy.\n",
        "\n",
        "### Key Features\n",
        "> - Language-flexible summarization\n",
        "- Progressive density improvement\n",
        "- Entity-focused refinement\n",
        "- Length consistency\n",
        "- Structured output format\n",
        "\n",
        "### Use Cases\n",
        "> - Multilingual document summarization\n",
        "- Cross-language content analysis\n",
        "- International news processing\n",
        "- Global research synthesis\n",
        "- Localized content optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku2VJunbZo7y"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_title = \"chain-of-density-multilingual\"\n",
        "\n",
        "chain_density_multilingual = \"\"\"\n",
        "Article: {ARTICLE}\n",
        "Language: {LANGUAGE}\n",
        "\n",
        "You will generate increasingly concise, entity-dense summaries of the above article in the specified language.\n",
        "\n",
        "Repeat the following 2 steps 5 times.\n",
        "\n",
        "Step 1. Identify 1-3 informative entities (\";\" delimited) from the article which are missing from the previously generated summary.\n",
        "Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the missing entities.\n",
        "\n",
        "A missing entity is:\n",
        "- relevant to the main story,\n",
        "- specific yet concise (100 words or fewer),\n",
        "- novel (not in the previous summary),\n",
        "- faithful (present in the article),\n",
        "- anywhere (can be located anywhere in the article).\n",
        "\n",
        "Guidelines:\n",
        "- The first summary should be long (8-10 sentences, ~200 words) yet highly non-specific\n",
        "- Make every word count: rewrite the previous summary to improve flow\n",
        "- Make space with fusion, compression, and removal of uninformative phrases\n",
        "- The summaries should become highly dense and concise yet self-contained\n",
        "- Missing entities can appear anywhere in the new summary\n",
        "- Never drop entities from the previous summary\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "[\n",
        "    {\n",
        "        \"Missing_Entities\": str,\n",
        "        \"Denser_Summary\": str\n",
        "    }\n",
        "]\n",
        "\n",
        "Provide the output in the specified language: {LANGUAGE}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(chain_density_multilingual)\n",
        "\n",
        "# Usage Example:\n",
        "response = chain_density_multilingual.format(\n",
        "    ARTICLE=\"Your article text here\",\n",
        "    LANGUAGE=\"Spanish\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdRjE4nIZvJN"
      },
      "source": [
        "### 1.2. Chain of Density Map (Multilingual)\n",
        "\n",
        "### Purpose\n",
        "> Create mapped summaries with increasing density in any specified language, focusing on key entity extraction and relationship mapping.\n",
        "\n",
        "### Key Features\n",
        "> - Language-adaptive processing\n",
        "- Three-step iteration process\n",
        "- Entity mapping functionality\n",
        "- Text format output\n",
        "- Entity relationship preservation\n",
        "\n",
        "### Use Cases\n",
        "> - Multi-language document analysis\n",
        "- Cross-cultural content mapping\n",
        "- International information structuring\n",
        "- Global text analysis\n",
        "- Multilingual content optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CUIIIcsZl2C"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_title = \"chain-of-density-map-multilingual\"\n",
        "\n",
        "chain_density_map_multilingual = \"\"\"\n",
        "Article: {ARTICLE}\n",
        "Language: {LANGUAGE}\n",
        "\n",
        "You will generate increasingly concise, entity-dense summaries of the above article in the specified language.\n",
        "\n",
        "Repeat the following 2 steps 3 times.\n",
        "\n",
        "Step 1. Identify 1-3 informative entities (\";\" delimited) from the article which are missing from the previous summary.\n",
        "Step 2. Write a new, denser summary of identical length covering all previous entities plus new ones.\n",
        "\n",
        "A missing entity is:\n",
        "- relevant to the main story,\n",
        "- specific yet concise (100 words or fewer),\n",
        "- novel (not in the previous summary),\n",
        "- faithful (present in the article),\n",
        "- anywhere (can be located anywhere in the article).\n",
        "\n",
        "Guidelines:\n",
        "- First summary: 8-10 sentences (~200 words), non-specific with fillers\n",
        "- Optimize word usage and improve flow\n",
        "- Remove uninformative phrases\n",
        "- Maintain density and self-containment\n",
        "- Preserve all previous entities\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "Text format for \"Missing Entities\" and \"Denser_Summary\"\n",
        "\n",
        "Provide the output in the specified language: {LANGUAGE}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(chain_density_map_multilingual)\n",
        "\n",
        "# Usage Example:\n",
        "response_map = chain_density_map_multilingual.format(\n",
        "    ARTICLE=\"Your article text here\",\n",
        "    LANGUAGE=\"Japanese\"  # or any other language\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ9zblwWWyZb"
      },
      "source": [
        "\n",
        "\n",
        "## 2. Key Information Extraction\n",
        "\n",
        "### Purpose\n",
        "> Extract and structure critical information from various document types with high precision and consistency.\n",
        "\n",
        "### Key Features\n",
        "> - Targeted data extraction\n",
        "- Structured output format\n",
        "- Context preservation\n",
        "- Pattern recognition\n",
        "- Validation checks\n",
        "\n",
        "### Use Cases\n",
        "> - Contract analysis\n",
        "- Financial report processing\n",
        "- Research data extraction\n",
        "- Document parsing\n",
        "- Information mining\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cVW1iVKQXBT7",
        "outputId": "63c7c080-194d-4614-878a-4d42948e85d1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://smith.langchain.com/prompts/key-information-extraction/b36be8df?organizationId=f2bffb3c-dd45-53ac-b23b-5d696451d11c'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain import hub\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_title = \"key-information-extraction\"\n",
        "\n",
        "extraction_prompt = \"\"\"\n",
        "Extract key information from the provided document according to these specifications:\n",
        "\n",
        "INPUT:\n",
        "- Document: {document}\n",
        "- Target Fields: {fields}\n",
        "- Context Requirements: {context}\n",
        "\n",
        "EXTRACTION REQUIREMENTS:\n",
        "1. Identify specified data points\n",
        "2. Maintain contextual relationships\n",
        "3. Validate extracted information\n",
        "4. Format according to schema\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "{\n",
        "    \"extracted_data\": dict,\n",
        "    \"confidence_scores\": dict,\n",
        "    \"validation_results\": dict,\n",
        "    \"metadata\": dict\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(extraction_prompt)\n",
        "hub.push(prompt_title, prompt, new_repo_is_public=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw0SbKcKD7xd"
      },
      "source": [
        "\n",
        "## 3. Metadata Tagging\n",
        "\n",
        "### Purpose\n",
        "> Automatically generate relevant tags and metadata to enhance content organization and searchability.\n",
        "\n",
        "### Key Features\n",
        "> - Automated tag generation\n",
        "- Hierarchical categorization\n",
        "- Search optimization\n",
        "- Content classification\n",
        "- Relationship mapping\n",
        "\n",
        "### Use Cases\n",
        "> - Content cataloging\n",
        "- Digital asset management\n",
        "- Document classification\n",
        "- Search enhancement\n",
        "- Content organization\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "wvAw2hW-Y_yH",
        "outputId": "9f28672a-0d08-438c-b64a-6c7c3fd9d347"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://smith.langchain.com/prompts/metadata-tagger/9bf50dec?organizationId=f2bffb3c-dd45-53ac-b23b-5d696451d11c'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain import hub\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_title = \"metadata-tagger\"\n",
        "\n",
        "metadata_prompt = \"\"\"\n",
        "Generate comprehensive metadata tags for the given content:\n",
        "\n",
        "CONTENT PARAMETERS:\n",
        "- Type: {content_type}\n",
        "- Domain: {domain}\n",
        "- Context: {context}\n",
        "\n",
        "TAGGING REQUIREMENTS:\n",
        "1. Generate relevant tags\n",
        "2. Create hierarchical categories\n",
        "3. Identify key topics\n",
        "4. Map relationships\n",
        "5. Optimize for search\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "{\n",
        "    \"primary_tags\": list,\n",
        "    \"categories\": dict,\n",
        "    \"relationships\": dict,\n",
        "    \"search_terms\": list\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(metadata_prompt)\n",
        "hub.push(prompt_title, prompt, new_repo_is_public=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4sJJp68D7xe"
      },
      "source": [
        "---\n",
        "# Specialized Prompts\n",
        "\n",
        "## 1. RAG Prompts\n",
        "\n",
        "### 1.1. RAG Document Analysis\n",
        "\n",
        "### Purpose\n",
        "> Process and answer questions based on retrieved document contexts with high accuracy and relevance.\n",
        "\n",
        "### Key Features\n",
        "> - Contextual understanding\n",
        "- Precise answer generation\n",
        "- Answer validation\n",
        "- Technical term preservation\n",
        "- Clear response formatting\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNn4Hc95ars2",
        "outputId": "94e67ea9-9bd5-4d5f-9f03-1e145852cbb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"You are a precise and helpful AI assistant specializing in question-answering tasks based on provided context.\\nYour primary task is to:\\n1. Analyze the provided context thoroughly\\n2. Answer questions using ONLY the information from the context\\n3. Preserve technical terms and proper nouns exactly as they appear\\n4. If the answer cannot be found in the context, respond with: 'The provided context does not contain information to answer this question.'\\n5. Format responses in clear, readable paragraphs with relevant examples when available\\n6. Focus on accuracy and clarity in your responses\\n\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='#Question:\\n{question}\\n\\n#Context:\\n{context}\\n\\n#Answer:\\nPlease provide a focused, accurate response that directly addresses the question using only the information from the provided context.'), additional_kwargs={})])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain import hub\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_title = \"rag-document-analysis\"\n",
        "\n",
        "system = \"\"\"You are a precise and helpful AI assistant specializing in question-answering tasks based on provided context.\n",
        "Your primary task is to:\n",
        "1. Analyze the provided context thoroughly\n",
        "2. Answer questions using ONLY the information from the context\n",
        "3. Preserve technical terms and proper nouns exactly as they appear\n",
        "4. If the answer cannot be found in the context, respond with: 'The provided context does not contain information to answer this question.'\n",
        "5. Format responses in clear, readable paragraphs with relevant examples when available\n",
        "6. Focus on accuracy and clarity in your responses\n",
        "\"\"\"\n",
        "\n",
        "human = \"\"\"#Question:\n",
        "{question}\n",
        "\n",
        "#Context:\n",
        "{context}\n",
        "\n",
        "#Answer:\n",
        "Please provide a focused, accurate response that directly addresses the question using only the information from the provided context.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system),\n",
        "    (\"human\", human)\n",
        "])\n",
        "\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "saj8jooLD7xe",
        "outputId": "506cffcd-4cc4-4908-9525-bf86b22ad5f6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://smith.langchain.com/prompts/rag-document-analysis/f7a42fa8?organizationId=f2bffb3c-dd45-53ac-b23b-5d696451d11c'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hub.push(prompt_title, prompt, new_repo_is_public=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd7KRy1-D7xe"
      },
      "source": [
        "### 1.2. RAG with Source Attribution\n",
        "\n",
        "### Purpose\n",
        "> Enhanced RAG implementation with detailed source tracking and citation for improved accountability and verification.\n",
        "\n",
        "### Key Features\n",
        "> - Source tracking\n",
        "- Citation inclusion\n",
        "- Context relevance scoring\n",
        "- Technical accuracy\n",
        "- Transparent attribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JTXOYs0ma5mu",
        "outputId": "b9dffe05-e619-4669-a8a6-660c42ce8c49"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://smith.langchain.com/prompts/rag-with-sources/67246bf3?organizationId=f2bffb3c-dd45-53ac-b23b-5d696451d11c'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain import hub\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_title = \"rag-with-sources\"\n",
        "\n",
        "system = \"\"\"You are a precise and thorough AI assistant that provides well-documented answers with source attribution.\n",
        "Your responsibilities include:\n",
        "1. Analyzing provided context thoroughly\n",
        "2. Generating accurate answers based solely on the given context\n",
        "3. Including specific source references for each key point\n",
        "4. Preserving technical terminology exactly as presented\n",
        "5. Maintaining clear citation format [source: page/document]\n",
        "6. If information is not found in the context, state: 'The provided context does not contain information to answer this question.'\n",
        "\n",
        "Format your response as:\n",
        "1. Main Answer\n",
        "2. Sources Used (with specific locations)\n",
        "3. Confidence Level (High/Medium/Low)\"\"\"\n",
        "\n",
        "human = \"\"\"#Question:\n",
        "{question}\n",
        "\n",
        "#Context:\n",
        "{context}\n",
        "\n",
        "#Answer:\n",
        "Please provide a detailed response with source citations using only information from the provided context.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system),\n",
        "    (\"human\", human)\n",
        "])\n",
        "PROMPT_OWNER='eun'\n",
        "hub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt, new_repo_is_public=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7OCErMJD7xe"
      },
      "source": [
        "## 2. LLM Response Evaluation\n",
        "\n",
        "### Purpose\n",
        "> Comprehensive evaluation of LLM responses based on multiple quality metrics with detailed scoring methodology.\n",
        "\n",
        "### Key Features\n",
        "> - Multi-dimensional assessment\n",
        "- Quantitative scoring\n",
        "- Qualitative analysis\n",
        "- Context relevance checking\n",
        "- Response accuracy validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7-eJ3XOa9dM"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_title = \"llm-response-evaluation\"\n",
        "\n",
        "evaluation_prompt = \"\"\"Evaluate the LLM's response based on the following criteria:\n",
        "\n",
        "INPUT:\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "LLM Response: {answer}\n",
        "\n",
        "EVALUATION CRITERIA:\n",
        "1. Accuracy (0-10)\n",
        "- Perfect (10): Completely accurate, perfectly aligned with context\n",
        "- Good (7-9): Minor inaccuracies\n",
        "- Fair (4-6): Some significant inaccuracies\n",
        "- Poor (0-3): Major inaccuracies or misalignment\n",
        "\n",
        "2. Completeness (0-10)\n",
        "- Perfect (10): Comprehensive coverage of all relevant points\n",
        "- Good (7-9): Covers most important points\n",
        "- Fair (4-6): Missing several key points\n",
        "- Poor (0-3): Critically incomplete\n",
        "\n",
        "3. Context Relevance (0-10)\n",
        "- Perfect (10): Optimal use of context\n",
        "- Good (7-9): Good use with minor omissions\n",
        "- Fair (4-6): Partial use of relevant context\n",
        "- Poor (0-3): Poor context utilization\n",
        "\n",
        "4. Clarity (0-10)\n",
        "- Perfect (10): Exceptionally clear and well-structured\n",
        "- Good (7-9): Clear with minor issues\n",
        "- Fair (4-6): Somewhat unclear\n",
        "- Poor (0-3): Confusing or poorly structured\n",
        "\n",
        "SCORING METHOD:\n",
        "1. Calculate individual scores\n",
        "2. Compute weighted average:\n",
        "   - Accuracy: 40%\n",
        "   - Completeness: 25%\n",
        "   - Context Relevance: 25%\n",
        "   - Clarity: 10%\n",
        "3. Normalize to 0-1 scale\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "{\n",
        "    \"individual_scores\": {\n",
        "        \"accuracy\": float,\n",
        "        \"completeness\": float,\n",
        "        \"context_relevance\": float,\n",
        "        \"clarity\": float\n",
        "    },\n",
        "    \"weighted_score\": float,\n",
        "    \"normalized_score\": float,\n",
        "    \"evaluation_notes\": string\n",
        "}\n",
        "\n",
        "Return ONLY the normalized_score as a decimal between 0 and 1.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(evaluation_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hQt-fczC_V2"
      },
      "source": [
        "---\n",
        "## Professional Domain Prompts\n",
        "\n",
        "Each professional domain prompt is carefully crafted to address specific industry needs and requirements.\n",
        "\n",
        "This part requires optimization of prompts, especially according to domain data and format. Therefore, it is recommended that you test multiple prompts with Playground on websites such as OpenAI or Anthropic and use the most appropriate prompts. Below is an example of prompts in each field.\n",
        "\n",
        "### Academic Research Analysis Prompt\n",
        "```python\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "As an expert academic researcher, analyze the academic content with:\n",
        "\n",
        "INPUT:\n",
        "- Content Type: {content_type}\n",
        "- Field of Study: {field}\n",
        "- Analysis Depth: {depth}\n",
        "\n",
        "ANALYZE:\n",
        "1. Research methodology and design\n",
        "2. Key findings and significance\n",
        "3. Theoretical framework\n",
        "4. Statistical validity\n",
        "5. Study limitations\n",
        "6. Future directions\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "{\n",
        "    \"executive_summary\": str,\n",
        "    \"methodology_analysis\": dict,\n",
        "    \"findings_analysis\": dict,\n",
        "    \"quality_assessment\": dict\n",
        "}\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "### 6.2. Clinical Case Analysis Prompt\n",
        "```python\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "As a medical professional, analyze clinical cases with:\n",
        "\n",
        "INPUT:\n",
        "- Patient Information: {patient_data}\n",
        "- Clinical Notes: {clinical_notes}\n",
        "\n",
        "PROVIDE:\n",
        "1. Clinical Assessment\n",
        "2. Diagnostic Process\n",
        "3. Treatment Plan\n",
        "4. Risk Assessment\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "{\n",
        "    \"clinical_summary\": str,\n",
        "    \"differential_diagnosis\": list,\n",
        "    \"treatment_plan\": dict,\n",
        "    \"risk_assessment\": dict\n",
        "}\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "### 6.3. Market Research Analysis Prompt\n",
        "```python\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "As a market research analyst, analyze:\n",
        "\n",
        "PARAMETERS:\n",
        "- Industry: {industry}\n",
        "- Market Segment: {segment}\n",
        "- Region: {region}\n",
        "- Time Period: {time_period}\n",
        "\n",
        "COMPONENTS:\n",
        "1. Market Overview\n",
        "2. Competitive Analysis\n",
        "3. Customer Analysis\n",
        "4. SWOT Analysis\n",
        "5. Financial Analysis\n",
        "6. Recommendations\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "{\n",
        "    \"market_overview\": dict,\n",
        "    \"competitive_landscape\": dict,\n",
        "    \"customer_insights\": dict,\n",
        "    \"strategic_recommendations\": list\n",
        "}\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "### 6.4. Educational Content Development Prompt\n",
        "```python\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "As an educational content developer, create:\n",
        "\n",
        "PARAMETERS:\n",
        "- Subject: {subject}\n",
        "- Grade Level: {grade_level}\n",
        "- Learning Objectives: {objectives}\n",
        "- Duration: {duration}\n",
        "\n",
        "DELIVER:\n",
        "1. Course Structure\n",
        "2. Learning Materials\n",
        "3. Assessment Components\n",
        "4. Differentiation Strategies\n",
        "5. Support Resources\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "{\n",
        "    \"course_outline\": dict,\n",
        "    \"lesson_plans\": list,\n",
        "    \"assessments\": dict,\n",
        "    \"support_resources\": dict\n",
        "}\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "### 6.5. Legal Document Analysis Prompt\n",
        "```python\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "As a legal professional, analyze:\n",
        "\n",
        "PARAMETERS:\n",
        "- Document Type: {doc_type}\n",
        "- Jurisdiction: {jurisdiction}\n",
        "- Legal Domain: {domain}\n",
        "\n",
        "ANALYZE:\n",
        "1. Document Overview\n",
        "2. Key Provisions\n",
        "3. Risk Assessment\n",
        "4. Compliance Check\n",
        "5. Recommendations\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "{\n",
        "    \"document_summary\": str,\n",
        "    \"key_provisions\": dict,\n",
        "    \"risk_analysis\": dict,\n",
        "    \"recommendations\": list\n",
        "}\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "### 6.6. UX Research Analysis Prompt\n",
        "```python\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "As a UX researcher, analyze:\n",
        "\n",
        "PARAMETERS:\n",
        "- Research Type: {research_type}\n",
        "- Product/Service: {product}\n",
        "- User Segment: {segment}\n",
        "- Research Goals: {goals}\n",
        "\n",
        "PROVIDE:\n",
        "1. User Behavior Analysis\n",
        "2. Usability Assessment\n",
        "3. User Experience Mapping\n",
        "4. Accessibility Evaluation\n",
        "5. Recommendations\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "{\n",
        "    \"behavioral_insights\": dict,\n",
        "    \"usability_metrics\": dict,\n",
        "    \"experience_mapping\": dict,\n",
        "    \"design_recommendations\": list\n",
        "}\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "### 6.7. Environmental Impact Assessment Prompt\n",
        "```python\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "As an environmental specialist, assess:\n",
        "\n",
        "PARAMETERS:\n",
        "- Project Type: {project_type}\n",
        "- Location: {location}\n",
        "- Scale: {scale}\n",
        "- Duration: {duration}\n",
        "\n",
        "ANALYZE:\n",
        "1. Environmental Baseline\n",
        "2. Impact Analysis\n",
        "3. Resource Assessment\n",
        "4. Mitigation Strategies\n",
        "5. Monitoring Plan\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "{\n",
        "    \"assessment_summary\": str,\n",
        "    \"impact_analysis\": dict,\n",
        "    \"mitigation_plan\": dict,\n",
        "    \"monitoring_framework\": dict\n",
        "}\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "Each prompt features:\n",
        "- Clear role definition\n",
        "- Specific input parameters\n",
        "- Structured requirements\n",
        "- Standardized JSON output format\n",
        "- Domain-specific considerations\n",
        "- Quality control measures"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py-test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
