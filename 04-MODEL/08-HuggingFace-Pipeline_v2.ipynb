{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "635d8ebb",
   "metadata": {},
   "source": [
    "# HuggingFace Pipeline\n",
    "\n",
    "- Author: [Sunworl](https://github.com/sunworl)\n",
    "- Design: [Teddy](https://github.com/teddylee777)\n",
    "- Peer Review: [Teddy](https://github.com/teddylee777)\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/sub-graph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239937-lesson-2-sub-graphs)\n",
    "\n",
    "## Hugging Face Local Pipeline\n",
    "\n",
    "You can run the Hugging Face model locally via the class  `HuggingFacePipeline`.\n",
    "\n",
    "The Hugging Face model Hub hosts over 120,000 models, 20,000 datasets, and 50,000 demo apps (Spaces) on its online platform, all of which are open-source and publicly available, allowing people to easily collaborate and build ML together.\n",
    "\n",
    "These models can be used in LangChain either by calling them through this local pipeline wrapper or by calling hosted inference endpoints through the HuggingFaseHub class. For more information on hosted pipelines, please refer to the HuggingFaseHub notebook.\n",
    "\n",
    "To use this, you should have the Python package transformers installed along with PyTorch.\n",
    "\n",
    "Additionally, you may install xformers for a more memory-efficient attention implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21943adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quit transformers --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b89cb",
   "metadata": {},
   "source": [
    "Set the path to download the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad1056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to download Hugging Face models/tokenizers\n",
    "# (Example)\n",
    "import os\n",
    "\n",
    "# ./cache/ Set to download to the specified path\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"./cache/\"\n",
    "os.environ[\"HF_HOME\"] = \"./cache/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7aba4",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "Models can be loaded by specifying model parameters using the method `from_model_id`.\n",
    "\n",
    "\n",
    "- The `langchain-opentutorial` class is used to load a pre-trained model from Hugging Face.\n",
    "\n",
    "- The `from_model_id` method is used to specify the `beomi/llama-2-ko-7b` model and set the task to \"text-generation\".\n",
    "\n",
    "- The `pipeline_kwargs` parameter is used to limit the maximum number of tokens to be generated to 10.\n",
    "\n",
    "- The loaded model is assigned to the `hf` variable, which can be used to perform text generation tasks.\n",
    "\n",
    "The model used: https://huggingface.co/beomi/llama-2-ko-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25ec196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9065ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dounload the HuggingFace model\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "\n",
    "    model_id=\"beomi/llama-2-ko-7b\",  # Specify the ID of the model to use\n",
    "\n",
    "    task=\"text-generation\",  # Specify the task to perform. Here, it's text generation\n",
    "    \n",
    "    # Set additional arguments to pass to the pipeline. Here, we limit the maximum number of new tokens to 512\n",
    "    pipeline_kwargs={\"max_new_tokens\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a9ae0",
   "metadata": {},
   "source": [
    "You can also load by directly passing an existing `transformers` pipeline.\n",
    "\n",
    "The text ageneration model is implemented using HuggingFacePipeline.\n",
    "\n",
    "\n",
    "- `AutoTokenizer` and `AutoModelForCausalLM` are used to load the `beomi/llama-2-ko-7b` model and tokenizer.\n",
    "\n",
    "- The `pipeline` function is used to create a \"text-generation\" pipeline, setting up the model and tokenizer. The maximum number of generated tokens is limited to 10.\n",
    "\n",
    "- The `HuggingFacePipeline` class is used to create an `hf` object, and the generated pipeline is passed to it.\n",
    "\n",
    "\n",
    "Using this created `hf` object, you can perform text generation for a given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f99b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"beomi/llama-2-ko-7b\"  # Specify the ID of the model to use\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id\n",
    ")  # Load the tokenizer for the specified model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)  # Load the specified model\n",
    "\n",
    "# Create a text generation pipeline and set the maximum number of new tokens to be generated to 10\n",
    "pipe = pipeline(\"text-generation\", model=model,\n",
    "                tokenizer=tokenizer, max_new_tokens=512)\n",
    "\n",
    "# Create a HuggingFacePipeline object and pass the generated pipeline to it\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00c3f4",
   "metadata": {},
   "source": [
    "## Create Chain\n",
    "\n",
    "Once the model is loaded into memory, you can configure it with prompts to form a chain.\n",
    "\n",
    "\n",
    "- A prompt template defining the question and answer format is created using the `PromptTemplate` class.\n",
    "\n",
    "- Create a `chain` object by connecting the `prompt` object and the `hf` object in a pipeline.\n",
    "\n",
    "- Call the `chain.invoke()` method to generate and output an answer for the given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69cb77da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"Answer the following question in Korean.\n",
    "#Question: \n",
    "{question}\n",
    "\n",
    "#Answer: \"\"\"  # A template defining the question and answer format\n",
    "prompt = PromptTemplate.from_template(template)  # Create a prompt object using the template\n",
    "\n",
    "# Create a chain by connecting the prompt and the language model\n",
    "chain = prompt | hf | StrOutputParser()\n",
    "\n",
    "question = \"대한민국의 수도는 어디야?\"  # Define the question\n",
    "\n",
    "print(\n",
    "    chain.invoke({\"question\": question})\n",
    ")  # Call the chain to generate and output an answer to the question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2fc536",
   "metadata": {},
   "source": [
    "## GPU Inference\n",
    "\n",
    "When running on a GPU, you can specify the `device=n` parameter to place the model on a specific device.\n",
    "\n",
    "The default value is `-1`, which means inference is performed on the CPU.\n",
    "\n",
    "If you have multiple GPUs or if the model is too large for a single GPU, you can specify `device_map=\"auto\"`.\n",
    "\n",
    "In this case, the [Accelerate](https://huggingface.co/docs/accelerate/index) library is required and is used to automatically determine how to load the model weights.\n",
    "\n",
    "*Caution*: `device` and `device_map` should not be specified together, as this can cause unexpected behavior.\n",
    "\n",
    "\n",
    "\n",
    "- Load the `gpt2` model using `HuggingFacePipeline` and set the `device` parameter to 0 to run it on the GPU.\n",
    "\n",
    "- Limit the maximum number of tokens to be generated to 10 using the `pipeline_kwargs` parameter.\n",
    "\n",
    "- Connect the `prompt` and `gpu_llm` in a pipeline to create the `gpu_chain`.\n",
    "\n",
    "- Call the `gpu_chain.invoke()` method to generate and output an answer for the given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b78d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_llm = HuggingFacePipeline.from_model_id(\n",
    "    \n",
    "    model_id=\"beomi/llama-2-ko-7b\",  # Specify the ID of the model to be used\n",
    "\n",
    "    task=\"text-generation\",  # Set the task to be performed. In this case, it is text generation\n",
    "\n",
    "    # Specify the GPU device number to be used. Setting it to \"auto\" will utilize the accelerate library\n",
    "    device=0,\n",
    "\n",
    "    # Set additional arguments to be passed to the pipeline. In this case, limit the maximum number of tokens to be generated to 10\n",
    "    pipeline_kwargs={\"max_new_tokens\": 64},\n",
    ")\n",
    "\n",
    "gpu_chain = prompt | gpu_llm  # Connect the prompt and gpu_llm to create the gpu_chain\n",
    "\n",
    "# Create a chain by connecting the prompt and the language model\n",
    "gpu_chain = prompt | gpu_llm | StrOutputParser()\n",
    "\n",
    "question = \"대한민국의 수도는 어디야?\"  # Define the question\n",
    "\n",
    "# Invoke the chain to generate and output an answer to the question\n",
    "print(gpu_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4d831",
   "metadata": {},
   "source": [
    "## Batch GPU Inference\n",
    "\n",
    "When running on a GPU device, you can perform inference in batch mode on the GPU.\n",
    "\n",
    "\n",
    "- Load the `beomi/llama-2-ko-7b` model using `HuggingFacePipeline` and set it to run on the GPU.\n",
    "\n",
    "- When creating the `gpu_llm`, set the `batch_size` to 2, `temperature` to 0, and `max_length` to 64.\n",
    "\n",
    "- Connect the `prompt` and `gpu_llm` in a pipeline to create the `gpu_chain`, and set the end token to \"\\n\\n\".\n",
    "\n",
    "- Use `gpu_chain.batch()` to generate answers in parallel for the `questions` in the questions.\n",
    "\n",
    "- Wrap each answer with <answer> tags and separate each answer with a line break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0874c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_llm = HuggingFacePipeline.from_model_id(\n",
    "\n",
    "    model_id=\"beomi/llama-2-ko-7b\",  # Specify the ID of the model to be used\n",
    "\n",
    "    task=\"text-generation\",  # Set the task to be performed\n",
    "\n",
    "    device=0,  # Specify the GPU device number. -1 indicates CPU\n",
    "\n",
    "    batch_size=2,  # Adjust the batch size. Set it appropriately based on GPU memory and model size.\n",
    "\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"max_length\": 256,\n",
    "    },  # Set additional arguments to be passed to the model\n",
    "\n",
    ")\n",
    "\n",
    "# Create a chain by connecting the prompt and the language model\n",
    "gpu_chain = prompt | gpu_llm.bind(stop=[\"\\n\\n\"])\n",
    "\n",
    "questions = []\n",
    "for i in range(4):\n",
    "\n",
    "    # Generate a list of questions\n",
    "    questions.append({\"question\": f\"숫자 {i} 이 한글로 뭐에요?\"})\n",
    "\n",
    "answers = gpu_chain.batch(questions)  # Batch process the list of questions to generate answers\n",
    "\n",
    "for answer in answers:\n",
    "\n",
    "    print(answer)  # Output the generated answers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-lwwSZlnu-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
