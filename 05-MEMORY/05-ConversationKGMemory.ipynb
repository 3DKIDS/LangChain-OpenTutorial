{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "635d8ebb",
   "metadata": {},
   "source": [
    "# ConversationKGMemory\n",
    "\n",
    "- Author: [Secludor](https://github.com/Secludor)\n",
    "- Design: [Secludor](https://github.com/Secludor)\n",
    "- Peer Review: \n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/sub-graph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239937-lesson-2-sub-graphs)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Unlike Entity Memory, which manages information about entities in a key-value format for individual entities, `Conversation Knowledge Graph Memory` is a module that manages relationships between entities in a graph format.\n",
    "\n",
    "It extracts and structures **knowledge triplets** (subject-relationship-object) to identify and store complex relationships between entities, and allows exploration of entity connectivity through **graph structure**.\n",
    "\n",
    "This helps the model understand relationships between different entities and better respond to queries based on complex networks and historical context.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environement Setup](#environment-setup)\n",
    "- [Conversation Knowlege Graph Memory](#conversation-knowlege-graph-memory)\n",
    "- [Applying KG Memory to Chain](#applying-kg-memory-to-chain)\n",
    "- [Applying KG Memory with LCEL](#applying-kg-memory-with-lcel)\n",
    "\n",
    "### References\n",
    "\n",
    "- [LangChain Python API Reference>langchain-community: 0.3.13>memory>ConversationKGMemory](https://python.langchain.com/api_reference/community/memory/langchain_community.memory.kg.ConversationKGMemory.html)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7aba4",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "[return](#overview)\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21943adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "!pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f25ec196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_core\",\n",
    "        \"langchain_community\",\n",
    "        \"langchain_openai\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f9065ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"05-ConversationKGMemory\",  # title 과 동일하게 설정해 주세요\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a9ae0",
   "metadata": {},
   "source": [
    "You can alternatively set API keys such as `OPENAI_API_KEY` in a `.env` file and load them.\n",
    "\n",
    "[Note] This is not necessary if you've already set the required API keys in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f99b5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load API keys from .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616661ad",
   "metadata": {},
   "source": [
    "## Conversation Knowlege Graph Memory\n",
    "[return](#overview)\n",
    "\n",
    "`Conversation Knowledge Graph Memory` is a memory module that stores and manages information extracted from conversations in a graph structure. This example demonstrates the following key features:\n",
    "\n",
    "- Storing conversation context (`save_context`)\n",
    "- Extracting entities from current conversation (`get_current_entities`)\n",
    "- Extracting knowledge triplets (`get_knowledge_triplets`)\n",
    "- Retrieving stored memory (`load_memory_variables`)\n",
    "\n",
    "The following example shows the process of extracting entities and relationships from a conversation about a new designer, Shelly Kim, and storing them in a graph format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17efec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.memory.kg import ConversationKGMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e165b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "memory = ConversationKGMemory(llm=llm, return_messages=True)\n",
    "memory.save_context(\n",
    "    {\"input\": \"This is Shelly Kim who lives in Pangyo.\"},\n",
    "    {\"output\": \"Hello Shelly, nice to meet you! What kind of work do you do?\"},\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Shelly Kim is our company's new designer.\"},\n",
    "    {\n",
    "        \"output\": \"That's great! Welcome to our team. I hope you'll enjoy working with us.\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eae79db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shelly Kim']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.get_current_entities({\"input\": \"Who is Shelly Kim?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb102317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [KnowledgeTriple(subject='Shelly Kim', predicate='lives in', object_='Pangyo')],\n",
       " [KnowledgeTriple(subject='Shelly Kim', predicate='lives in', object_='Pangyo'),\n",
       "  KnowledgeTriple(subject='Shelly Kim', predicate='is', object_=\"company's new designer\")])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.get_knowledge_triplets({\"input\": \"Shelly\"}), memory.get_knowledge_triplets(\n",
    "    {\"input\": \"Pangyo\"}\n",
    "), memory.get_knowledge_triplets({\"input\": \"job\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92d6929d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content=\"On Shelly Kim: Shelly Kim lives in Pangyo. Shelly Kim is our company's new designer.\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"Who is Shelly Kim?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00c3f4",
   "metadata": {},
   "source": [
    "## Applying KG Memory to Chain\n",
    "[return](#overview)\n",
    "\n",
    "This section demonstrates how to use `ConversationKGMemory` with `ConversationChain`\n",
    "\n",
    "(The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. if you want, skip to [Apply to LCEL](applying-kg-memory-with-lcel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b020b140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Caelu\\AppData\\Local\\Temp\\ipykernel_3664\\900596707.py:21: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation_with_kg = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.memory.kg import ConversationKGMemory\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides lots of specific details from its context. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know. \n",
    "The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
    "\n",
    "Relevant Information:\n",
    "\n",
    "{history}\n",
    "\n",
    "Conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "\n",
    "conversation_with_kg = ConversationChain(\n",
    "    llm=llm, prompt=prompt, memory=ConversationKGMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf33db4",
   "metadata": {},
   "source": [
    "Let's initialize the conversation with some basic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e25d23b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Teddy! It's great to meet you. It sounds like Shelly is embarking on an exciting journey as a new designer at your company. Design roles can be incredibly rewarding, as they often involve a mix of creativity, problem-solving, and collaboration. If there's anything specific you'd like to discuss about Shelly's role or how you can support her as a coworker, feel free to let me know!\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_kg.predict(\n",
    "    input=\"My name is Teddy. Shelly is a coworker of mine, and she's a new designer at our company.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d10e7",
   "metadata": {},
   "source": [
    "Let's query memory for information about Shelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0e71b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'On Shelly: Shelly is a coworker of Teddy. Shelly is a new designer. Shelly works at our company.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_kg.memory.load_memory_variables({\"input\": \"who is Shelly?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc034d9b",
   "metadata": {},
   "source": [
    "You can also reset the memory by `memory.clear()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "208b5e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': ''}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_kg.memory.clear()\n",
    "conversation_with_kg.memory.load_memory_variables({\"input\": \"who is Shelly?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d42f5",
   "metadata": {},
   "source": [
    "## Applying KG Memory with LCEL\n",
    "[return](#overview)\n",
    "\n",
    "Let's examine the memory after having a conversation using custom `ConversationChain` with `ConversationKGMemory` by LCEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1905d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_community.memory.kg import ConversationKGMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"The following is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides lots of specific details from its context. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know. \n",
    "The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
    "\n",
    "Relevant Information:\n",
    "{history}\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationKGMemory(llm=llm, return_messages=True, memory_key=\"history\")\n",
    "\n",
    "\n",
    "class ConversationChain:\n",
    "    def __init__(self, prompt, llm, memory):\n",
    "        self.memory = memory\n",
    "        self.chain = (\n",
    "            RunnablePassthrough()\n",
    "            | RunnablePassthrough.assign(\n",
    "                history=RunnableLambda(memory.load_memory_variables)\n",
    "                | itemgetter(\"history\")\n",
    "            )\n",
    "            | prompt\n",
    "            | llm\n",
    "        )\n",
    "\n",
    "    def invoke(self, input_dict):\n",
    "        response = self.chain.invoke(input_dict)\n",
    "        self.memory.save_context(input_dict, {\"output\": response.content})\n",
    "        return response\n",
    "\n",
    "\n",
    "conversation_with_kg = ConversationChain(prompt, llm, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ca6c16",
   "metadata": {},
   "source": [
    "Let's initialize the conversation with some basic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99ac927b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Teddy! It's nice to meet you. It sounds like you and Shelly are working together at your company. How's everything going with the new designer on board?\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = conversation_with_kg.invoke(\n",
    "    {\n",
    "        \"input\": \"My name is Teddy. Shelly is a coworker of mine, and she's a new designer at our company.\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514ea4d8",
   "metadata": {},
   "source": [
    "Let's query memory for information about Shelly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c18820be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Shelly: Shelly is a coworker of Teddy. Shelly is a new designer. Shelly works at our company.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_kg.memory.load_memory_variables({\"input\": \"who is Shelly?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d527d",
   "metadata": {},
   "source": [
    "You can also reset the memory by `memory.clear()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dfac660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': []}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_kg.memory.clear()\n",
    "conversation_with_kg.memory.load_memory_variables({\"input\": \"who is Shelly?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-A2cWC-y3-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
