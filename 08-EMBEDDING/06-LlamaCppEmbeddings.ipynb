{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LlamaCpp Embeddings With Langchain\n",
        "\n",
        "- Author: [Yongdam Kim](https://github.com/dancing-with-coffee/)\n",
        "- Design: []()\n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/sub-graph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239937-lesson-2-sub-graphs)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial covers how to perform **Text Embedding** using **Llama-cpp** and **Langchain**.\n",
        "\n",
        "**Llama-cpp** is an open-source package implemented in C++ that allows you to use LLMs such as llama very efficiently locally.\n",
        "\n",
        "In this tutorial, we will create a simple example to measure similarity between `Documents` and an input `Query` using **Llama-cpp** and **Langchain**.\n",
        "\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environment Setup](#environment-setup)\n",
        "- [Llama-cpp Installation and Model Serving](#llama-cpp-installation-and-model-serving)\n",
        "- [Identify Supported Embedding Models and Serving Model](#identify-supported-embedding-models-and-serving-model)\n",
        "- [Model Load and Embedding](#model-load-and-embedding)\n",
        "- [The similarity calculation results](#the-similarity-calculation-results)\n",
        "\n",
        "### References\n",
        "\n",
        "- [Llama-cpp Python](https://github.com/abetlen/llama-cpp-python)\n",
        "- [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
        "\n",
        "**[Note]**\n",
        "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials.\n",
        "- You can check out the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution_count": 1
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution_count": 2
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langchain_community\",\n",
        "        \"llama-cpp-python\",\n",
        "        \"scikit-learn\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")\n",
        "\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Llama-cpp Installation and Model Serving\n",
        "\n",
        "Llama-cpp is an open-source project that makes it easy to run large language models (LLMs) locally. It allows you to download and run various LLMs on your own computer, giving you the freedom to experiment with AI models.\n",
        "\n",
        "To install **llama-cpp-python**:\n",
        "```bash\n",
        "pip install llama-cpp-python\n",
        "```\n",
        "\n",
        "1. Make sure you have the required environment for C++ compilation (e.g., on Linux or macOS). \n",
        "2. Download or specify your chosen LLaMA model file (e.g., `Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf`).\n",
        "3. Check that `llama-cpp-python` can find the model path.\n",
        "\n",
        "Below, we will demonstrate how to serve a LLaMA model using Llama-cpp. You can follow the official [llama-cpp-python documentation](https://github.com/abetlen/llama-cpp-python) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Identify Supported Embedding Models and Serving Model\n",
        "\n",
        "You can find a variety of LLaMA-based models, which typically come in different quantizations (e.g., q4_0, q4_1, q5_0, q8_0, etc.).\n",
        "\n",
        "**1. Search models**\n",
        "- You can look for models on Hugging Face or other community websites.\n",
        "\n",
        "**2. Download or Pull a Model**\n",
        "- For instance, you could download from Hugging Face if the model is hosted.\n",
        "\n",
        "**3. Verify the Model**\n",
        "- Check that the `.bin` (or `.gguf`) file is accessible to your environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Load and Embedding\n",
        "\n",
        "Now that you have installed `llama-cpp-python` and have downloaded a model, let's see how to load it and use it for text embedding.\n",
        "\n",
        "Below, we define a `Query` or some `Documents` to embed using `Llama-cpp` within LangChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution_count": 3
      },
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import LlamaCppEmbeddings\n",
        "\n",
        "# Example query and documents\n",
        "query = \"Langchain is awesome!\"\n",
        "docs = [\n",
        "    \"Spaghetti Carbonara is a traditional Italian pasta dish made with eggs, cheese, pancetta, and pepper. It's simple yet incredibly delicious. Typically served with spaghetti, but can also be enjoyed with other pasta types.\",\n",
        "    \"The tropical island of Bali offers stunning beaches, volcanic mountains, lush forests, and vibrant coral reefs. Travelers often visit for surfing, yoga retreats, and the unique Balinese Hindu culture.\",\n",
        "    \"C++ is a high-performance programming language widely used in system/software development, game programming, and real-time simulations. It supports both procedural and object-oriented paradigms.\",\n",
        "    \"In astronomy, the Drake Equation is a probabilistic argument used to estimate the number of active, communicative extraterrestrial civilizations in the Milky Way galaxy. It takes into account factors such as star formation rate and fraction of habitable planets.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the Embedding Model\n",
        "\n",
        "Below is how you can initialize the `LlamaCppEmbeddings` class by specifying the path to your LLaMA model file (`model_path`).\n",
        "\n",
        "For example, you might have a downloaded model path: `../../../llama-cpp-embeddings/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf`.\n",
        "\n",
        "We demonstrate how to instantiate the embeddings class and then embed queries and documents using Llama-cpp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution_count": 7
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_load_model_from_file: using device Metal (Apple M2 Ultra) - 98303 MiB free\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from ../../../llama-cpp-embeddings/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
            "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
            "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
            "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\n",
            "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\n",
            "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
            "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "ggml_backend_metal_log_allocated_size: allocated buffer, size =  4685.33 MiB, ( 4685.39 / 98304.00)\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading output layer to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors: Metal_Mapped model buffer size =  4685.31 MiB\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =   281.81 MiB\n",
            ".......................................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 512\n",
            "llama_new_context_with_model: n_ctx_per_seq = 512\n",
            "llama_new_context_with_model: n_batch       = 512\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "ggml_metal_init: allocating\n",
            "ggml_metal_init: found device: Apple M2 Ultra\n",
            "ggml_metal_init: picking default device: Apple M2 Ultra\n",
            "ggml_metal_init: using embedded metal library\n",
            "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
            "ggml_metal_init: simdgroup reduction   = true\n",
            "ggml_metal_init: simdgroup matrix mul. = true\n",
            "ggml_metal_init: has bfloat            = true\n",
            "ggml_metal_init: use bfloat            = false\n",
            "ggml_metal_init: hasUnifiedMemory      = true\n",
            "ggml_metal_init: recommendedMaxWorkingSetSize  = 103079.22 MB\n",
            "ggml_metal_init: loaded kernel_add                                    0x119511820 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row                                0x119517b80 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sub                                    0x10ba1bb30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sub_row                                0x10c65ff50 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul                                    0x119405170 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_row                                0x10703ebe0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_div                                    0x12edaf830 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_div_row                                0x1297619b0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_f32                             0x119518340 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_f16                             0x12edaf230 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_i32                             0x119518ae0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_i16                             0x1099041e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_scale                                  0x119a8e440 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_scale_4                                0x11931a740 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_clamp                                  0x119516530 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_tanh                                   0x119a8f460 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_relu                                   0x119405c50 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sigmoid                                0x119a8eed0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu                                   0x119a901a0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_4                                 0x12edae2e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_quick                             0x119406410 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_quick_4                           0x11931b5e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_silu                                   0x12edb0b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_silu_4                                 0x119a91080 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_elu                                    0x109905e60 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f16                           0x1297611e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x119a91630 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f32                           0x129760920 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x119a91bc0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_diag_mask_inf                          0x119407040 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1297635d0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_f32                           0x119407740 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_f16                           0x12edaece0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x119a91e20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129763030 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x119407ed0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x119519a90 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x119404a00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x129761490 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12edb18d0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x119a921f0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x129764e40 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x129765180 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x119408500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12edb26e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x129765810 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1195190a0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x119a93620 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x119a93900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x119a93b60 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x129765a70 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x119a94420 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_i32                           0x119519300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rms_norm                               0x1297660e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_group_norm                             0x12edb1240 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_norm                                   0x109905580 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x109906460 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x129766340 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x119a93dc0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11951ae60 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11951b490 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x119409580 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x119a94d60 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12edb2fa0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12edb36e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x119a94fc0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x119a95340 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129766be0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x109904d50 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x119409ec0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x119a95670 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x119a95f40 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12edb3da0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11951bd70 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11951b6f0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11940a6d0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x119a96460 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11931bb70 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12edb4480 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129766e40 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x119319380 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1297677f0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11951c4b0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x119a97010 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x109904fb0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1297670a0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11931c180 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x119408d80 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x119a97270 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x129768140 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11931b0b0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12edb5300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12edb59c0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x119408fe0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x129768bc0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11951d5d0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11931a1e0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12edb5fb0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11951dd10 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12edb66f0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12edb6df0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11931d150 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12edb7510 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11931e760 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11951cd20 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12edb7c50 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x119a97630 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x109907050 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11951eb90 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1297693d0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1297699a0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10d304440 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107679230 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x109907c30 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x105433510 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x119a97890 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1070416f0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1070445c0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10ba1c4d0 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107044fa0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10c660700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10c660e40 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x129e89120 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12e993920 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12e992d20 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x129e89660 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10c6611c0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x119a98490 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x107040460 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12e9942e0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10c661ad0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1054323a0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10703f400 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x129e8a1f0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x107045e40 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12e993000 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12e995140 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x107046b20 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10c662310 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x129e8a9a0 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10c661420 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105434960 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12e9958b0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12e9965e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12e996d40 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12e997400 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x129769c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x109907900 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12e996020 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1070472c0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10c662d10 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10c663630 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12e997a10 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x107047910 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1054314f0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1070460a0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12e998b70 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12edb85c0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12edb8dd0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x119a98fd0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12edb94c0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11940c4e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11951f1a0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1054359e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107048810 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11940cc60 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11951f400 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12edb9c50 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11951f840 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12edba3c0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12edbab10 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11940d3c0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12edbb1e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12edbb790 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11951ff70 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x119520f60 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12edbbee0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x119521690 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x119521e20 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1195225a0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12edbc670 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11940d620 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x119522ca0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1195233b0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11940da20 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_norm_f32                          0x119523b20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_norm_f16                          0x119524250 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_neox_f32                          0x119524930 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_neox_f16                          0x12edbcdc0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_f16                             0x119524fd0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_f32                             0x119a98be0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1195257e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x119525f50 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x119526bd0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12edbd520 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_upscale_f32                            0x11931f160 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pad_f32                                0x12edbdc70 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x119526e30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12edbe3c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_arange_f32                             0x11940ea50 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12edbee80 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12edbf5e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x119527670 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11940f1a0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12edbfd50 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11940f8f0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12edc04e0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x119410090 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1194107b0 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x129769e60 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1195278d0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x119410da0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11931fa50 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x119527ff0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x119411550 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x119411cc0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x119a99700 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1054305e0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12976a630 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12edc0c30 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1195288d0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x119a9a0d0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12edc0e90 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x119a9a450 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x119320270 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1194123e0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12976a9b0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x119a99960 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12edc10f0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x105436080 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12976afc0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x119529150 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x119a9a9b0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x119a9b540 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x119a9b120 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1099084e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x119412640 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12e997e50 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x109906d70 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10c663ff0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12976b780 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10c6644f0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12e998dd0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x119320a20 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12976c790 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x119412980 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x119529970 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x119413740 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x119412c80 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x105436870 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x119a9ccd0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_f32                                0x119414750 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_i32                                0x12e999200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x119a9d440 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x119a9da10 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
            "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107679930 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10771b8e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10990a330 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149e28e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12e999460 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149e4d7a0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x107048ed0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10990aa30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_concat                                 0x119a9ddb0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sqr                                    0x10ba1dbd0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sqrt                                   0x10990b2d0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sin                                    0x107049780 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cos                                    0x149e4e960 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sum_rows                               0x10771a4d0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argmax                                 0x12e99a660 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12edc1350 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x129e8b690 | th_max = 1024 | th_width =   32\n",
            "llama_kv_cache_init:      Metal KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.02 MiB\n",
            "llama_new_context_with_model:      Metal compute buffer size =   258.50 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =     9.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'quantize.imatrix.file': '/models_out/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct.imatrix', 'quantize.imatrix.chunks_count': '125', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.type': 'model', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.model': 'gpt2', 'llama.embedding_length': '4096', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'general.finetune': 'Instruct', 'general.file_type': '15', 'llama.block_count': '32', 'general.size_label': '8B', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.feed_forward_length': '14336', 'general.quantization_version': '2', 'llama.rope.dimension_count': '128', 'general.license': 'llama3.1', 'llama.attention.head_count': '32', 'quantize.imatrix.entries_count': '224', 'llama.context_length': '131072', 'general.architecture': 'llama', 'general.basename': 'Meta-Llama-3.1', 'llama.rope.freq_base': '500000.000000', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'general.name': 'Meta Llama 3.1 8B Instruct'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{- bos_token }}\n",
            "{%- if custom_tools is defined %}\n",
            "    {%- set tools = custom_tools %}\n",
            "{%- endif %}\n",
            "{%- if not tools_in_user_message is defined %}\n",
            "    {%- set tools_in_user_message = true %}\n",
            "{%- endif %}\n",
            "{%- if not date_string is defined %}\n",
            "    {%- set date_string = \"26 Jul 2024\" %}\n",
            "{%- endif %}\n",
            "{%- if not tools is defined %}\n",
            "    {%- set tools = none %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
            "{%- if messages[0]['role'] == 'system' %}\n",
            "    {%- set system_message = messages[0]['content']|trim %}\n",
            "    {%- set messages = messages[1:] %}\n",
            "{%- else %}\n",
            "    {%- set system_message = \"\" %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- System message + builtin tools #}\n",
            "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
            "{%- if builtin_tools is defined or tools is not none %}\n",
            "    {{- \"Environment: ipython\\n\" }}\n",
            "{%- endif %}\n",
            "{%- if builtin_tools is defined %}\n",
            "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
            "{%- endif %}\n",
            "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
            "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
            "{%- if tools is not none and not tools_in_user_message %}\n",
            "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "{%- endif %}\n",
            "{{- system_message }}\n",
            "{{- \"<|eot_id|>\" }}\n",
            "\n",
            "{#- Custom tools are passed in a user message with some extra guidance #}\n",
            "{%- if tools_in_user_message and not tools is none %}\n",
            "    {#- Extract the first user message so we can plug it in here #}\n",
            "    {%- if messages | length != 0 %}\n",
            "        {%- set first_user_message = messages[0]['content']|trim %}\n",
            "        {%- set messages = messages[1:] %}\n",
            "    {%- else %}\n",
            "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
            "{%- endif %}\n",
            "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
            "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
            "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "    {{- first_user_message + \"<|eot_id|>\"}}\n",
            "{%- endif %}\n",
            "\n",
            "{%- for message in messages %}\n",
            "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
            "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
            "    {%- elif 'tool_calls' in message %}\n",
            "        {%- if not message.tool_calls|length == 1 %}\n",
            "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
            "        {%- endif %}\n",
            "        {%- set tool_call = message.tool_calls[0].function %}\n",
            "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
            "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
            "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
            "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
            "                {%- if not loop.last %}\n",
            "                    {{- \", \" }}\n",
            "                {%- endif %}\n",
            "                {%- endfor %}\n",
            "            {{- \")\" }}\n",
            "        {%- else  %}\n",
            "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
            "            {{- '\"parameters\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- \"}\" }}\n",
            "        {%- endif %}\n",
            "        {%- if builtin_tools is defined %}\n",
            "            {#- This means we're in ipython mode #}\n",
            "            {{- \"<|eom_id|>\" }}\n",
            "        {%- else %}\n",
            "            {{- \"<|eot_id|>\" }}\n",
            "        {%- endif %}\n",
            "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
            "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
            "        {%- if message.content is mapping or message.content is iterable %}\n",
            "            {{- message.content | tojson }}\n",
            "        {%- else %}\n",
            "            {{- message.content }}\n",
            "        {%- endif %}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
            "{%- endif %}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding model has been successfully loaded.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Using chat eos_token: <|eot_id|>\n",
            "Using chat bos_token: <|begin_of_text|>\n"
          ]
        }
      ],
      "source": [
        "# Load the Llama-cpp Embedding Model\n",
        "model_path = \"../../../llama-cpp-embeddings/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\"  # example path\n",
        "\n",
        "embedder = LlamaCppEmbeddings(model_path=model_path, n_gpu_layers=-1)\n",
        "print(\"Embedding model has been successfully loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embedding Queries and Documents\n",
        "\n",
        "Now let's embed both the `query` and the `documents`. We will verify the dimension of the output vectors.\n",
        "\n",
        "However, there is currently one issue that cannot be resolved when using the latest model with `LlamaCppEmbeddings`. I will post the link to the issue below, so please check it out and if it is resolved in the latest version, you can use it as instructed in the original langchain official tutorial.\n",
        "\n",
        "- Issue link : https://github.com/langchain-ai/langchain/issues/22532"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution_count": 8
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_load_model_from_file: using device Metal (Apple M2 Ultra) - 93290 MiB free\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from ../../../llama-cpp-embeddings/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
            "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
            "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
            "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\n",
            "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\n",
            "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
            "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "ggml_backend_metal_log_allocated_size: allocated buffer, size =  4685.33 MiB, ( 9699.03 / 98304.00)\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading output layer to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors: Metal_Mapped model buffer size =  4685.31 MiB\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =   281.81 MiB\n",
            ".......................................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 512\n",
            "llama_new_context_with_model: n_ctx_per_seq = 512\n",
            "llama_new_context_with_model: n_batch       = 512\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "ggml_metal_init: allocating\n",
            "ggml_metal_init: found device: Apple M2 Ultra\n",
            "ggml_metal_init: picking default device: Apple M2 Ultra\n",
            "ggml_metal_init: using embedded metal library\n",
            "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
            "ggml_metal_init: simdgroup reduction   = true\n",
            "ggml_metal_init: simdgroup matrix mul. = true\n",
            "ggml_metal_init: has bfloat            = true\n",
            "ggml_metal_init: use bfloat            = false\n",
            "ggml_metal_init: hasUnifiedMemory      = true\n",
            "ggml_metal_init: recommendedMaxWorkingSetSize  = 103079.22 MB\n",
            "ggml_metal_init: loaded kernel_add                                    0x12e9c3d10 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row                                0x119a9f210 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sub                                    0x119a9ea50 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sub_row                                0x13e904560 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul                                    0x129ffee20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_row                                0x119a9fe10 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_div                                    0x119306410 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_div_row                                0x13e904a90 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_f32                             0x129fff520 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_f16                             0x119304740 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_i32                             0x13e905150 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_i16                             0x119aa0490 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_scale                                  0x148b2d870 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_scale_4                                0x13f9fcf30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_clamp                                  0x119305ec0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_tanh                                   0x129ffdcc0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_relu                                   0x119aa0c60 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sigmoid                                0x13ea38b40 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu                                   0x13ea39470 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_4                                 0x109909d70 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_quick                             0x13f9fc8f0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_quick_4                           0x119aa1280 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_silu                                   0x119aa1df0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_silu_4                                 0x148b2e930 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_elu                                    0x10990be30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f16                           0x148b2e470 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x148b2fd20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f32                           0x13e906460 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x129dd5880 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_diag_mask_inf                          0x119aa22a0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10704a920 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_f32                           0x119aa2ce0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_f16                           0x129fffca0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e906be0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10990ce10 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ea39a80 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ea3a090 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ea3abb0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x119aa3010 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e907210 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x129deff20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e9078d0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x129df1b60 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x119aa3a70 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e907f50 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e908670 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1396063c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1396069d0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x148b2ff80 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x119aa4160 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x148b30460 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x119aa46d0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_i32                           0x13e908d90 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rms_norm                               0x119aa4cf0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_group_norm                             0x119aa53b0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_norm                                   0x119aa5ad0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x119aa6010 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x148b30d20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ea3b3b0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ea3bcd0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139606d50 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x148b31330 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x148b319c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10990d250 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1193070e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10990dff0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x119aa6270 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x148b32070 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x148b31c20 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e909390 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x119aa64d0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x119aa7060 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e909900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x119aa77e0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ea3c6b0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ea3a6e0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ea3d630 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ea3dda0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1396072b0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ea3e500 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x139607ac0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e90a1c0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x139608250 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x119aa68b0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x119aa7f30 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x119aa87e0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x119aa8e20 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e90b500 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10ba1fca0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x149e4fdc0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x129ed8170 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10c640f40 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10704ac50 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10c63ef80 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x149e50020 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10ba1ff60 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10704b380 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x108a69730 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x108aceac0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x129ed9b60 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12e9c4950 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x108a72260 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x148b33190 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12e9c3690 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10704bdd0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x119aa9580 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10c65a330 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10c65a590 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149e50490 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12e9c5090 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10704c4e0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10990da50 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12e9c54c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10704c740 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12e9c6750 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10c65a7f0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x148b339f0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12e9c6e90 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12e9c5720 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12e9c5ec0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129ed96e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10c65aa50 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10c65acb0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x105437820 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x105436d90 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10704d210 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x129eda830 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149e50cc0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x119aa9d50 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ea04300 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ea04a70 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1396089b0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x119307980 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ea051e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1396090f0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ea05a40 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x119aaa490 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11930d030 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10990ed40 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ea06320 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ea06a70 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1396096f0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x139609ee0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e90b760 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ea07190 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1193087a0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x148b340d0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10990f4d0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e90a950 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13960a620 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x148b349c0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x119aaaac0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x105436ff0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x148b34cf0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ea077d0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e90c510 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x148b35380 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x148b35990 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x148b360d0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13960add0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ea07a30 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e90c770 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x129df46c0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x129df4d50 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ea07da0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10990f730 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11930e740 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11930d740 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x119aab080 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ea08f60 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11930f4e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x148b36330 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e90d3b0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x119aab6b0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11930ee70 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e90e770 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e90eee0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x119aaba80 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x148b36760 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129df57e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ea085d0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11930f0d0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e90f650 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e90f8b0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e90fb10 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e910180 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ea09760 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ea0aa40 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ea0b030 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_f16                             0x13e910860 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_f32                             0x148b36e80 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x119aac1c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x148b370e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e911050 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e911700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_upscale_f32                            0x13ea0b830 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pad_f32                                0x13e911df0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13e9124a0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x119310af0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_arange_f32                             0x13e913170 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ea0c1b0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e9138b0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1193421d0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x119aac940 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ea0c5b0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x119342c80 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x119aad080 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e913b10 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x119342ee0 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ea0ce30 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e9142d0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e914a40 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x129df5b10 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x148b37460 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x119aad7c0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129df62c0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e9152f0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x119aade90 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e915a00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x148b37970 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x119343140 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x119aae510 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e915c60 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x119aaec20 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x148b38320 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10990fab0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e915ec0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e9161e0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x119aaee80 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x119aaf0e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e916a00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x119aaf720 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x129df6750 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x119aafd60 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1054380d0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x119ab0100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ea0d580 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x148b38bd0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x119ab0730 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x148b38e30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e916d20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e917540 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x129df7300 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x119343fd0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x129df76a0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e918560 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x119343770 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x129df6fd0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129df8dc0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x119344870 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x119345fa0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_f32                                0x13e9191f0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_i32                                0x13e919880 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129df9510 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x119346710 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
            "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x119ab14e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x119ab0bc0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x148b39090 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12e9c7640 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x119ab2170 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129df9bd0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x119ab1a50 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x119ab26e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_concat                                 0x13e91a120 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sqr                                    0x13e91a450 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sqrt                                   0x12e9c8bb0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sin                                    0x119ab2940 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cos                                    0x119ab3560 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sum_rows                               0x129dfa2e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argmax                                 0x13e91aaf0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x148b395c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e91b330 | th_max = 1024 | th_width =   32\n",
            "llama_kv_cache_init:      Metal KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.02 MiB\n",
            "llama_new_context_with_model:      Metal compute buffer size =   258.50 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =     9.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'quantize.imatrix.file': '/models_out/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct.imatrix', 'quantize.imatrix.chunks_count': '125', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.type': 'model', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.model': 'gpt2', 'llama.embedding_length': '4096', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'general.finetune': 'Instruct', 'general.file_type': '15', 'llama.block_count': '32', 'general.size_label': '8B', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.feed_forward_length': '14336', 'general.quantization_version': '2', 'llama.rope.dimension_count': '128', 'general.license': 'llama3.1', 'llama.attention.head_count': '32', 'quantize.imatrix.entries_count': '224', 'llama.context_length': '131072', 'general.architecture': 'llama', 'general.basename': 'Meta-Llama-3.1', 'llama.rope.freq_base': '500000.000000', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'general.name': 'Meta Llama 3.1 8B Instruct'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{- bos_token }}\n",
            "{%- if custom_tools is defined %}\n",
            "    {%- set tools = custom_tools %}\n",
            "{%- endif %}\n",
            "{%- if not tools_in_user_message is defined %}\n",
            "    {%- set tools_in_user_message = true %}\n",
            "{%- endif %}\n",
            "{%- if not date_string is defined %}\n",
            "    {%- set date_string = \"26 Jul 2024\" %}\n",
            "{%- endif %}\n",
            "{%- if not tools is defined %}\n",
            "    {%- set tools = none %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
            "{%- if messages[0]['role'] == 'system' %}\n",
            "    {%- set system_message = messages[0]['content']|trim %}\n",
            "    {%- set messages = messages[1:] %}\n",
            "{%- else %}\n",
            "    {%- set system_message = \"\" %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- System message + builtin tools #}\n",
            "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
            "{%- if builtin_tools is defined or tools is not none %}\n",
            "    {{- \"Environment: ipython\\n\" }}\n",
            "{%- endif %}\n",
            "{%- if builtin_tools is defined %}\n",
            "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
            "{%- endif %}\n",
            "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
            "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
            "{%- if tools is not none and not tools_in_user_message %}\n",
            "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "{%- endif %}\n",
            "{{- system_message }}\n",
            "{{- \"<|eot_id|>\" }}\n",
            "\n",
            "{#- Custom tools are passed in a user message with some extra guidance #}\n",
            "{%- if tools_in_user_message and not tools is none %}\n",
            "    {#- Extract the first user message so we can plug it in here #}\n",
            "    {%- if messages | length != 0 %}\n",
            "        {%- set first_user_message = messages[0]['content']|trim %}\n",
            "        {%- set messages = messages[1:] %}\n",
            "    {%- else %}\n",
            "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
            "{%- endif %}\n",
            "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
            "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
            "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "    {{- first_user_message + \"<|eot_id|>\"}}\n",
            "{%- endif %}\n",
            "\n",
            "{%- for message in messages %}\n",
            "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
            "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
            "    {%- elif 'tool_calls' in message %}\n",
            "        {%- if not message.tool_calls|length == 1 %}\n",
            "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
            "        {%- endif %}\n",
            "        {%- set tool_call = message.tool_calls[0].function %}\n",
            "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
            "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
            "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
            "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
            "                {%- if not loop.last %}\n",
            "                    {{- \", \" }}\n",
            "                {%- endif %}\n",
            "                {%- endfor %}\n",
            "            {{- \")\" }}\n",
            "        {%- else  %}\n",
            "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
            "            {{- '\"parameters\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- \"}\" }}\n",
            "        {%- endif %}\n",
            "        {%- if builtin_tools is defined %}\n",
            "            {#- This means we're in ipython mode #}\n",
            "            {{- \"<|eom_id|>\" }}\n",
            "        {%- else %}\n",
            "            {{- \"<|eot_id|>\" }}\n",
            "        {%- endif %}\n",
            "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
            "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
            "        {%- if message.content is mapping or message.content is iterable %}\n",
            "            {{- message.content | tojson }}\n",
            "        {%- else %}\n",
            "            {{- message.content }}\n",
            "        {%- endif %}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "Using chat eos_token: <|eot_id|>\n",
            "Using chat bos_token: <|begin_of_text|>\n",
            "llama_perf_context_print:        load time =     180.66 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     180.91 ms /     7 tokens\n",
            "llama_perf_context_print:        load time =     180.66 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    45 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     118.37 ms /    46 tokens\n",
            "llama_perf_context_print:        load time =     180.66 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    40 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     116.81 ms /    41 tokens\n",
            "llama_perf_context_print:        load time =     180.66 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     115.75 ms /    36 tokens\n",
            "llama_perf_context_print:        load time =     180.66 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    49 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     118.31 ms /    50 tokens\n"
          ]
        }
      ],
      "source": [
        "# from langchain tutorial\n",
        "\n",
        "\"\"\"\n",
        "embedded_query = llama_embeddings.embed_query(query)\n",
        "embedded_docs = llama_embeddings.embed_documents(docs)\n",
        "\n",
        "print(f\"Embedding Dimension Output (Query): {len(embedded_query)}\")\n",
        "print(f\"Embedding Dimension Output (Docs): {len(embedded_docs[0])}\")\n",
        "\"\"\"\n",
        "\n",
        "# Overridden version of the LlamaCppEmbeddings class\n",
        "from typing import List\n",
        "from langchain_community.llms.llamacpp import LlamaCpp\n",
        "from langchain_community.embeddings.llamacpp import LlamaCppEmbeddings\n",
        "\n",
        "\n",
        "class CustomLlamaCppEmbeddings(LlamaCppEmbeddings):\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Embed a list of documents using the Llama model.\n",
        "\n",
        "        Args:\n",
        "            texts: The list of texts to embed.\n",
        "\n",
        "        Returns:\n",
        "            List of embeddings, one for each text.\n",
        "        \"\"\"\n",
        "        embeddings = [self.client.embed(text)[0] for text in texts]\n",
        "        return [list(map(float, e)) for e in embeddings]\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"Embed a query using the Llama model.\n",
        "\n",
        "        Args:\n",
        "            text: The text to embed.\n",
        "\n",
        "        Returns:\n",
        "            Embeddings for the text.\n",
        "        \"\"\"\n",
        "        embedding = self.client.embed(text)[0]\n",
        "        return list(map(float, embedding))\n",
        "\n",
        "\n",
        "c_embedder = CustomLlamaCppEmbeddings(model_path=model_path, n_gpu_layers=-1)\n",
        "embedded_query = c_embedder.embed_query(query)\n",
        "embedded_docs = c_embedder.embed_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check custom embeddings\n",
        "\n",
        "- To check whether the embedding results are output as expected, I output the dimensions of each embedding vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution_count": 11
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query embedding dimension: 4096\n",
            "Document embedding dimension: 4096\n"
          ]
        }
      ],
      "source": [
        "print(\"Query embedding dimension:\", len(embedded_query))\n",
        "print(\"Document embedding dimension:\", len(embedded_docs[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The similarity calculation results\n",
        "\n",
        "We can use the vector representations of the query and documents to calculate similarity.\n",
        "Here, we use the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) provided by scikit-learn.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution_count": 9
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.99999991 0.99999991 0.99999991 0.99999991]\n",
            "[Query] Langchain is awesome!\n",
            "====================================\n",
            "[0] similarity: 1.000 | In astronomy, the Drake Equation is a probabilistic argument used to estimate the number of active, communicative extraterrestrial civilizations in the Milky Way galaxy. It takes into account factors such as star formation rate and fraction of habitable planets.\n",
            "\n",
            "[1] similarity: 1.000 | C++ is a high-performance programming language widely used in system/software development, game programming, and real-time simulations. It supports both procedural and object-oriented paradigms.\n",
            "\n",
            "[2] similarity: 1.000 | The tropical island of Bali offers stunning beaches, volcanic mountains, lush forests, and vibrant coral reefs. Travelers often visit for surfing, yoga retreats, and the unique Balinese Hindu culture.\n",
            "\n",
            "[3] similarity: 1.000 | Spaghetti Carbonara is a traditional Italian pasta dish made with eggs, cheese, pancetta, and pepper. It's simple yet incredibly delicious. Typically served with spaghetti, but can also be enjoyed with other pasta types.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Calculate Cosine Similarity\n",
        "similarities = cosine_similarity([embedded_query], embedded_docs)[0]\n",
        "print(similarities)\n",
        "\n",
        "# Sort indices in ascending order.\n",
        "sorted_indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "print(f\"[Query] {query}\\n====================================\")\n",
        "for i, idx in enumerate(sorted_indices):\n",
        "    print(f\"[{i}] similarity: {similarities[idx]:.3f} | {docs[idx]}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "- [Llama-cpp Python GitHub](https://github.com/abetlen/llama-cpp-python)\n",
        "- [LangChain Documentation](https://langchain.readthedocs.io/en/latest/)\n",
        "- [Cosine Similarity - Wikipedia](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
        "----\n",
        "This concludes the **Llama-cpp Embeddings With Langchain** tutorial in the style of the original reference notebook."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-tutorial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
