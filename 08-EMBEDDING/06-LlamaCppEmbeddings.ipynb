{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LlamaCpp Embeddings With Langchain\n",
        "\n",
        "- Author: [Yongdam Kim](https://github.com/dancing-with-coffee/)\n",
        "- Design: []()\n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/sub-graph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239937-lesson-2-sub-graphs)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial covers how to perform **Text Embedding** using **Llama-cpp** and **Langchain**.\n",
        "\n",
        "**Llama-cpp** is an open-source package implemented in C++ that allows you to use LLMs such as llama very efficiently locally.\n",
        "\n",
        "In this tutorial, we will create a simple example to measure similarity between `Documents` and an input `Query` using **Llama-cpp** and **Langchain**.\n",
        "\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environment Setup](#environment-setup)\n",
        "- [Llama-cpp Installation and Model Serving](#llama-cpp-installation-and-model-serving)\n",
        "- [Identify Supported Embedding Models and Serving Model](#identify-supported-embedding-models-and-serving-model)\n",
        "- [Model Load and Embedding](#model-load-and-embedding)\n",
        "- [The similarity calculation results](#the-similarity-calculation-results)\n",
        "\n",
        "### References\n",
        "\n",
        "- [Llama-cpp Python](https://github.com/abetlen/llama-cpp-python)\n",
        "- [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
        "\n",
        "**[Note]**\n",
        "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials.\n",
        "- You can check out the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution_count": 1
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution_count": 2
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment variables have been set successfully.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "from langchain_opentutorial import set_env\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "set_env(\n",
        "    {\n",
        "        \"OPENAI_API_KEY\": \"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\",\n",
        "        \"LANGCHAIN_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "        \"LANGCHAIN_PROJECT\": \"OpenAI-Embeddings\",\n",
        "    },\n",
        ")\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langchain_community\",\n",
        "        \"llama-cpp-python\",\n",
        "        \"scikit-learn\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")\n",
        "\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Llama-cpp Installation and Model Serving\n",
        "\n",
        "Llama-cpp is an open-source project that makes it easy to run large language models (LLMs) locally. It allows you to download and run various LLMs on your own computer, giving you the freedom to experiment with AI models.\n",
        "\n",
        "To install **llama-cpp-python**:\n",
        "```bash\n",
        "pip install llama-cpp-python\n",
        "```\n",
        "\n",
        "1. Make sure you have the required environment for C++ compilation (e.g., on Linux or macOS). \n",
        "2. Download or specify your chosen LLaMA model file (e.g., `Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf`).\n",
        "3. Check that `llama-cpp-python` can find the model path.\n",
        "\n",
        "Below, we will demonstrate how to serve a LLaMA model using Llama-cpp. You can follow the official [llama-cpp-python documentation](https://github.com/abetlen/llama-cpp-python) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Identify Supported Embedding Models and Serving Model\n",
        "\n",
        "You can find a variety of LLaMA-based models, which typically come in different quantizations (e.g., q4_0, q4_1, q5_0, q8_0, etc.).\n",
        "\n",
        "**1. Search models**\n",
        "- You can look for models on Hugging Face or other community websites.\n",
        "\n",
        "**2. Download or Pull a Model**\n",
        "- For instance, you could download from Hugging Face if the model is hosted.\n",
        "\n",
        "**3. Verify the Model**\n",
        "- Check that the `.bin` (or `.gguf`) file is accessible to your environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Load and Embedding\n",
        "\n",
        "Now that you have installed `llama-cpp-python` and have downloaded a model, let's see how to load it and use it for text embedding.\n",
        "\n",
        "Below, we define a `Query` or some `Documents` to embed using `Llama-cpp` within LangChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution_count": 3
      },
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import LlamaCppEmbeddings\n",
        "\n",
        "# Example query and documents\n",
        "query = \"Langchain is awesome!\"\n",
        "docs = [\n",
        "    \"Spaghetti Carbonara is a traditional Italian pasta dish made with eggs, cheese, pancetta, and pepper. It's simple yet incredibly delicious. Typically served with spaghetti, but can also be enjoyed with other pasta types.\",\n",
        "    \"The tropical island of Bali offers stunning beaches, volcanic mountains, lush forests, and vibrant coral reefs. Travelers often visit for surfing, yoga retreats, and the unique Balinese Hindu culture.\",\n",
        "    \"C++ is a high-performance programming language widely used in system/software development, game programming, and real-time simulations. It supports both procedural and object-oriented paradigms.\",\n",
        "    \"In astronomy, the Drake Equation is a probabilistic argument used to estimate the number of active, communicative extraterrestrial civilizations in the Milky Way galaxy. It takes into account factors such as star formation rate and fraction of habitable planets.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the Embedding Model\n",
        "\n",
        "Below is how you can initialize the `LlamaCppEmbeddings` class by specifying the path to your LLaMA model file (`model_path`).\n",
        "\n",
        "For example, you might have a downloaded model path: `../../../llama-cpp-embeddings/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf`.\n",
        "\n",
        "We demonstrate how to instantiate the embeddings class and then embed queries and documents using Llama-cpp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution_count": 7
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_load_model_from_file: using device Metal (Apple M2 Ultra) - 98303 MiB free\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from ../../../llama-cpp-embeddings/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
            "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
            "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
            "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\n",
            "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\n",
            "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
            "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "ggml_backend_metal_log_allocated_size: allocated buffer, size =  4685.33 MiB, ( 4685.39 / 98304.00)\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading output layer to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors: Metal_Mapped model buffer size =  4685.31 MiB\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =   281.81 MiB\n",
            ".......................................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 512\n",
            "llama_new_context_with_model: n_ctx_per_seq = 512\n",
            "llama_new_context_with_model: n_batch       = 512\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "ggml_metal_init: allocating\n",
            "ggml_metal_init: found device: Apple M2 Ultra\n",
            "ggml_metal_init: picking default device: Apple M2 Ultra\n",
            "ggml_metal_init: using embedded metal library\n",
            "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
            "ggml_metal_init: simdgroup reduction   = true\n",
            "ggml_metal_init: simdgroup matrix mul. = true\n",
            "ggml_metal_init: has bfloat            = true\n",
            "ggml_metal_init: use bfloat            = false\n",
            "ggml_metal_init: hasUnifiedMemory      = true\n",
            "ggml_metal_init: recommendedMaxWorkingSetSize  = 103079.22 MB\n",
            "ggml_metal_init: loaded kernel_add                                    0x10601dfc0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row                                0x147df0750 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sub                                    0x13660c720 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sub_row                                0x147df0f90 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul                                    0x147df1310 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_row                                0x147defc20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_div                                    0x147df2260 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_div_row                                0x147df2a20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_f32                             0x1242dfa80 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_f16                             0x147df3270 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_i32                             0x147df39f0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_i16                             0x147df4430 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_scale                                  0x13660e070 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_scale_4                                0x147df4a60 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_clamp                                  0x1256af7a0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_tanh                                   0x147df56a0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_relu                                   0x147df6580 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sigmoid                                0x147df6fa0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu                                   0x147df7a00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_4                                 0x147df7fb0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_quick                             0x147df8c70 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_quick_4                           0x13660d850 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_silu                                   0x147df9360 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_silu_4                                 0x147df9e70 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_elu                                    0x147dfacd0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f16                           0x147dfa190 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x147dfa670 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f32                           0x147dfb800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13660ed30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_diag_mask_inf                          0x147dfbcb0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136457140 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_f32                           0x147dfbf10 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_f16                           0x13660f220 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x147dfcb60 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13625cdb0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x3343052a0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x147dfd8e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13660fe20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x147dfdce0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x147dfec70 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x147dff030 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127404080 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1366105f0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136610ca0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136611c70 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x147dffc70 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1366113e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127404e20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136612830 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127405280 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x334404540 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127405a20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_i32                           0x127406030 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rms_norm                               0x127406db0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_group_norm                             0x127407510 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_norm                                   0x136612030 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127406560 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136613260 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1366139e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127406980 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127407c30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127408f70 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1274097b0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127409f50 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12740a7d0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127408760 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136614240 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1274089c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12740bd90 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1366149f0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1366151e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136615990 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136616070 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136616820 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12740b4d0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136617030 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1366176f0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12740d490 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136618b70 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12740dcc0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1366194a0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136458060 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12740cdf0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136619fc0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12740e580 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1366181a0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12740f580 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136618400 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12740ee40 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12740f0a0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13661b8b0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13661a7b0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1274101e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13661c420 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127410920 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127411060 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x334305ab0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13661d010 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13661df30 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13661d8b0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13661c810 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127412420 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1274046e0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13661ee20 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127412f70 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13661fbc0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127413cc0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127413770 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1256afa00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127414620 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127414cb0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127415a40 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1274161f0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1242e0f70 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1256afd70 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1274155d0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1256affd0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1256b0480 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127416820 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13661f340 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127417e60 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13661f5a0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1366208c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10601fdc0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136621f30 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1256b0e20 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1274185a0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1364589b0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136621af0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136622af0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1274190b0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1366239f0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127418930 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1366234b0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127418c80 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12741a3e0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127419d10 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12741ae70 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x334104430 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12741b5b0 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1364594c0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136624380 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136624b80 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12741bca0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1256b15a0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10601e420 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136625730 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136626220 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136626e90 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12741bf00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1366274c0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136627980 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1366267c0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13625d690 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x106020e50 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136628940 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12741c510 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12741d690 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136629080 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136629810 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136629eb0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13662a230 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12741d260 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12741e240 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12741e9e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12741f0c0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13662a7c0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13662afa0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127420840 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1274210f0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1256b06e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13662bfd0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1274219a0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13662cd30 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127420030 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x334504880 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13662d440 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13662d9a0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13662e0c0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127422500 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127422cc0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13662f300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13662e830 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13662eca0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127423950 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_norm_f32                          0x127423200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_norm_f16                          0x127424170 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_neox_f32                          0x127424f10 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_neox_f16                          0x127424900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_f16                             0x1274255a0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_f32                             0x127425890 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127425cf0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127426660 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1256b1ff0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127427b70 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_upscale_f32                            0x127427f60 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pad_f32                                0x127428770 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127428220 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127429740 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_arange_f32                             0x1256b2250 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12742a0a0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136630790 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12742ace0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12742bd90 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136630fa0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12742b360 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12742b970 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1366317d0 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12742cee0 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1256b3390 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12742da70 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136631bf0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12742e2a0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12742e9c0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x3341049c0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12742f1d0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13645a6c0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12742f8f0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12742fdc0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1274303e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1274307b0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136633260 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136633ab0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1256b24b0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127430a10 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127432690 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136632530 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136633f40 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1256b3e80 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127433240 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127433a30 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1274341a0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127434400 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127434b20 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127435330 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1366359c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136636140 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x334204590 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x334204c40 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10622c040 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x3342048f0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x106252cd0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x334405430 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x334205d90 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x106251620 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x334207160 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x334207a30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x334208470 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x334208b90 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1062528b0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x106253e30 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_f32                                0x334209990 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_i32                                0x334505040 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x334209360 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x33420a8c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
            "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x33420b070 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x33420b810 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1062538c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x33420a0e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13625c550 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x33420c850 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x33420cf30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x33420c010 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_concat                                 0x334405ce0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sqr                                    0x33420dfd0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sqrt                                   0x106254790 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sin                                    0x33420da50 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cos                                    0x33420f300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sum_rows                               0x106255e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argmax                                 0x33420ec30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x334304d90 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x33420ee90 | th_max = 1024 | th_width =   32\n",
            "llama_kv_cache_init:      Metal KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.02 MiB\n",
            "llama_new_context_with_model:      Metal compute buffer size =   258.50 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =     9.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'quantize.imatrix.file': '/models_out/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct.imatrix', 'quantize.imatrix.chunks_count': '125', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.type': 'model', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.model': 'gpt2', 'llama.embedding_length': '4096', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'general.finetune': 'Instruct', 'general.file_type': '15', 'llama.block_count': '32', 'general.size_label': '8B', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.feed_forward_length': '14336', 'general.quantization_version': '2', 'llama.rope.dimension_count': '128', 'general.license': 'llama3.1', 'llama.attention.head_count': '32', 'quantize.imatrix.entries_count': '224', 'llama.context_length': '131072', 'general.architecture': 'llama', 'general.basename': 'Meta-Llama-3.1', 'llama.rope.freq_base': '500000.000000', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'general.name': 'Meta Llama 3.1 8B Instruct'}\n",
            "Available chat formats from metadata: chat_template.default\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding model has been successfully loaded.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using gguf chat template: {{- bos_token }}\n",
            "{%- if custom_tools is defined %}\n",
            "    {%- set tools = custom_tools %}\n",
            "{%- endif %}\n",
            "{%- if not tools_in_user_message is defined %}\n",
            "    {%- set tools_in_user_message = true %}\n",
            "{%- endif %}\n",
            "{%- if not date_string is defined %}\n",
            "    {%- set date_string = \"26 Jul 2024\" %}\n",
            "{%- endif %}\n",
            "{%- if not tools is defined %}\n",
            "    {%- set tools = none %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
            "{%- if messages[0]['role'] == 'system' %}\n",
            "    {%- set system_message = messages[0]['content']|trim %}\n",
            "    {%- set messages = messages[1:] %}\n",
            "{%- else %}\n",
            "    {%- set system_message = \"\" %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- System message + builtin tools #}\n",
            "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
            "{%- if builtin_tools is defined or tools is not none %}\n",
            "    {{- \"Environment: ipython\\n\" }}\n",
            "{%- endif %}\n",
            "{%- if builtin_tools is defined %}\n",
            "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
            "{%- endif %}\n",
            "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
            "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
            "{%- if tools is not none and not tools_in_user_message %}\n",
            "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "{%- endif %}\n",
            "{{- system_message }}\n",
            "{{- \"<|eot_id|>\" }}\n",
            "\n",
            "{#- Custom tools are passed in a user message with some extra guidance #}\n",
            "{%- if tools_in_user_message and not tools is none %}\n",
            "    {#- Extract the first user message so we can plug it in here #}\n",
            "    {%- if messages | length != 0 %}\n",
            "        {%- set first_user_message = messages[0]['content']|trim %}\n",
            "        {%- set messages = messages[1:] %}\n",
            "    {%- else %}\n",
            "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
            "{%- endif %}\n",
            "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
            "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
            "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "    {{- first_user_message + \"<|eot_id|>\"}}\n",
            "{%- endif %}\n",
            "\n",
            "{%- for message in messages %}\n",
            "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
            "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
            "    {%- elif 'tool_calls' in message %}\n",
            "        {%- if not message.tool_calls|length == 1 %}\n",
            "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
            "        {%- endif %}\n",
            "        {%- set tool_call = message.tool_calls[0].function %}\n",
            "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
            "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
            "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
            "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
            "                {%- if not loop.last %}\n",
            "                    {{- \", \" }}\n",
            "                {%- endif %}\n",
            "                {%- endfor %}\n",
            "            {{- \")\" }}\n",
            "        {%- else  %}\n",
            "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
            "            {{- '\"parameters\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- \"}\" }}\n",
            "        {%- endif %}\n",
            "        {%- if builtin_tools is defined %}\n",
            "            {#- This means we're in ipython mode #}\n",
            "            {{- \"<|eom_id|>\" }}\n",
            "        {%- else %}\n",
            "            {{- \"<|eot_id|>\" }}\n",
            "        {%- endif %}\n",
            "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
            "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
            "        {%- if message.content is mapping or message.content is iterable %}\n",
            "            {{- message.content | tojson }}\n",
            "        {%- else %}\n",
            "            {{- message.content }}\n",
            "        {%- endif %}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "Using chat eos_token: <|eot_id|>\n",
            "Using chat bos_token: <|begin_of_text|>\n"
          ]
        }
      ],
      "source": [
        "# Load the Llama-cpp Embedding Model\n",
        "model_path = \"../../../llama-cpp-embeddings/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\"  # example path\n",
        "\n",
        "embedder = LlamaCppEmbeddings(model_path=model_path, n_gpu_layers=-1)\n",
        "print(\"Embedding model has been successfully loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embedding Queries and Documents\n",
        "\n",
        "Now let's embed both the `query` and the `documents`. We will verify the dimension of the output vectors.\n",
        "\n",
        "However, there is currently one issue that cannot be resolved when using the latest model with `LlamaCppEmbeddings`. I will post the link to the issue below, so please check it out and if it is resolved in the latest version, you can use it as instructed in the original langchain official tutorial.\n",
        "\n",
        "- Issue link : https://github.com/langchain-ai/langchain/issues/22532"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution_count": 8
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_load_model_from_file: using device Metal (Apple M2 Ultra) - 93290 MiB free\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from ../../../llama-cpp-embeddings/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
            "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
            "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
            "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\n",
            "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\n",
            "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
            "llm_load_vocab: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
            "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "ggml_backend_metal_log_allocated_size: allocated buffer, size =  4685.33 MiB, ( 9699.03 / 98304.00)\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading output layer to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors: Metal_Mapped model buffer size =  4685.31 MiB\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =   281.81 MiB\n",
            ".......................................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 512\n",
            "llama_new_context_with_model: n_ctx_per_seq = 512\n",
            "llama_new_context_with_model: n_batch       = 512\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "ggml_metal_init: allocating\n",
            "ggml_metal_init: found device: Apple M2 Ultra\n",
            "ggml_metal_init: picking default device: Apple M2 Ultra\n",
            "ggml_metal_init: using embedded metal library\n",
            "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
            "ggml_metal_init: simdgroup reduction   = true\n",
            "ggml_metal_init: simdgroup matrix mul. = true\n",
            "ggml_metal_init: has bfloat            = true\n",
            "ggml_metal_init: use bfloat            = false\n",
            "ggml_metal_init: hasUnifiedMemory      = true\n",
            "ggml_metal_init: recommendedMaxWorkingSetSize  = 103079.22 MB\n",
            "ggml_metal_init: loaded kernel_add                                    0x1362d6bd0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row                                0x33431ca50 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sub                                    0x4ae947600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sub_row                                0x3342a1240 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul                                    0x1062be070 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_row                                0x4ae948a40 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_div                                    0x4ae947a70 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_div_row                                0x3342a1900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_f32                             0x1362d57b0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_f16                             0x1362d67c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_i32                             0x3342a02d0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_i16                             0x4aefb6270 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_scale                                  0x4aefb6bd0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_scale_4                                0x1362d5f20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_clamp                                  0x3342a0530 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_tanh                                   0x33431d8a0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_relu                                   0x1362d7760 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sigmoid                                0x3342a31d0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu                                   0x4aeb37570 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_4                                 0x33431d050 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_quick                             0x1362d8b70 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_quick_4                           0x4ae947db0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_silu                                   0x4ae949d80 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_silu_4                                 0x33431c0e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_elu                                    0x33431e870 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f16                           0x33411b0b0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x4aefb55c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f32                           0x33431ead0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x3342a3660 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_diag_mask_inf                          0x4aeb36f60 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x4ae94ac20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_f32                           0x4aefb5960 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_f16                           0x3342a3ca0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x4aeb384c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x33431ed30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1362d82e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x33431f000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x33431f380 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x4ae94afc0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x3342a3f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x4aefb7bc0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x4aefb82a0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x4aefb8960 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x3342a3900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x3342a5a70 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x4aefb9000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x4aefb97a0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x4ae94b680 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x4ae94bda0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x147ddc450 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x3342a6260 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1362d9570 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_i32                           0x4aeb39670 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rms_norm                               0x4ae94c410 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_group_norm                             0x4aeb39d30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_norm                                   0x4aeb3a6e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1362d97d0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x4aefb9e50 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x3343205d0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1362d9d00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1362da7a0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1362d9f60 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x33410b700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1362da1c0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1362daec0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x33431fb30 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x33411b310 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x4aefba630 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x4aeb3ae70 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x334320c30 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x334321410 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x33411b920 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1362db320 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x3342a6e00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x4aefba890 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x3342a7410 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x4aefbac80 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x4aefbbca0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x3342a7e10 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x4aeb39f90 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x4aeb3bcb0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x4aefbc410 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x3342a81e0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x3342a8900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x33411bb80 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x334321eb0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x4aeb3c4a0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1362dc600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1060121b0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1256e5ef0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1362dbd10 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13645ae70 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136690500 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136637cc0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13645bbf0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127437c30 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1366397e0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136690950 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1256e4860 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1061088c0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x106022330 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1366911d0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x147dcb780 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x147dd97a0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x33411bde0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136692de0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136692790 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13645c400 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13645cab0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13645d1f0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x147dec060 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x33411c220 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x147dcc3a0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x147dec2c0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136693ca0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136694400 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136694c20 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13645d7e0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x140726660 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x106022b40 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x147dcc6d0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1366953f0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1256e4060 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x147dd8750 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x147dcafa0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13645b180 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13645ec40 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x33411d110 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x147ddcc50 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x147dde0e0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1362dc090 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136695b00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1366961b0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13645f0c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1362dd3b0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x106108030 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13645f850 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13645ffc0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136696920 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x33411d370 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136460750 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1366970d0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136460f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1362ddbf0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x4aefbd670 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x334322810 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x4aefbdd50 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1362de700 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x4ae94c000 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13645dec0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x106109830 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x4aefbe3e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1256f2fa0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x147ddceb0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127440680 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x33411def0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x4aeb3cde0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x4aeb3d520 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x3342a92e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x4aeb3dcf0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x4ae94ca40 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x3342a9ad0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x3342aa200 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x4aeb3e740 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x33411d610 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x4aeb3ee70 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x4aeb3f520 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x4aeb3fc80 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x4aeb40450 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x4aeb40b60 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x4aeb412b0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x4aeb41a20 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x4aeb42190 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x3342aa8a0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x334322f60 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x4aeb42900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x4aeb42fb0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x4aefbe640 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x4aeb43740 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x334321670 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x4aeb43e20 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x4aeb445e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x4aeb44ce0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x334323db0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1362dec30 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_norm_f32                          0x3342aafe0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_norm_f16                          0x33411eb90 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_neox_f32                          0x33411edf0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_neox_f16                          0x1362df3c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_f16                             0x334324010 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_f32                             0x3342ab240 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x334324880 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x4aeb451e0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1362df620 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x4aeb45440 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_upscale_f32                            0x1362df910 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pad_f32                                0x4aeb46440 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x3342ab550 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x33411f1f0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_arange_f32                             0x334325350 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x3342ac240 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1362dfb70 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x4ae94da40 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1362e0990 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x4aefbefa0 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1362e0d90 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x106109050 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1362e1610 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x4ae94e0c0 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x3342ac860 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x3342acac0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x4aefbf750 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x4aefbfea0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x3342ace20 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x33411fc20 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x334325680 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x334120420 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x3342ae5f0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x4aefc05c0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x4aefc0d10 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x334325e30 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1362e01d0 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x3342af490 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x3342af950 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1362e2700 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x4aeb466a0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x4aeb46ad0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1362e2d40 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x334120680 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x3341208e0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x334120c30 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x3342afcd0 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x4ae94cd10 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1362e3850 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x4ae94f070 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136697840 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1256ef600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1362e3f10 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x334326580 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x3342aff30 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x106021a20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1362e4170 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127440990 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10610a660 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127440bf0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10610adc0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136461790 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1060231d0 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127440e50 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x334326dc0 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1362e43d0 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_f32                                0x136461ee0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_i32                                0x136462580 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1060214c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136462c80 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
            "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1274417d0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136463360 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1274410b0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127441e40 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x3341218a0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136697e20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136698520 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136698af0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_concat                                 0x1256c4c20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sqr                                    0x1242e0a50 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sqrt                                   0x106024940 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sin                                    0x136699340 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cos                                    0x1242e1be0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sum_rows                               0x136699a20 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argmax                                 0x13669a130 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1274420a0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x106024ba0 | th_max = 1024 | th_width =   32\n",
            "llama_kv_cache_init:      Metal KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.02 MiB\n",
            "llama_new_context_with_model:      Metal compute buffer size =   258.50 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =     9.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'quantize.imatrix.file': '/models_out/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct.imatrix', 'quantize.imatrix.chunks_count': '125', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.type': 'model', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.model': 'gpt2', 'llama.embedding_length': '4096', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'general.finetune': 'Instruct', 'general.file_type': '15', 'llama.block_count': '32', 'general.size_label': '8B', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.feed_forward_length': '14336', 'general.quantization_version': '2', 'llama.rope.dimension_count': '128', 'general.license': 'llama3.1', 'llama.attention.head_count': '32', 'quantize.imatrix.entries_count': '224', 'llama.context_length': '131072', 'general.architecture': 'llama', 'general.basename': 'Meta-Llama-3.1', 'llama.rope.freq_base': '500000.000000', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'general.name': 'Meta Llama 3.1 8B Instruct'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{- bos_token }}\n",
            "{%- if custom_tools is defined %}\n",
            "    {%- set tools = custom_tools %}\n",
            "{%- endif %}\n",
            "{%- if not tools_in_user_message is defined %}\n",
            "    {%- set tools_in_user_message = true %}\n",
            "{%- endif %}\n",
            "{%- if not date_string is defined %}\n",
            "    {%- set date_string = \"26 Jul 2024\" %}\n",
            "{%- endif %}\n",
            "{%- if not tools is defined %}\n",
            "    {%- set tools = none %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
            "{%- if messages[0]['role'] == 'system' %}\n",
            "    {%- set system_message = messages[0]['content']|trim %}\n",
            "    {%- set messages = messages[1:] %}\n",
            "{%- else %}\n",
            "    {%- set system_message = \"\" %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- System message + builtin tools #}\n",
            "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
            "{%- if builtin_tools is defined or tools is not none %}\n",
            "    {{- \"Environment: ipython\\n\" }}\n",
            "{%- endif %}\n",
            "{%- if builtin_tools is defined %}\n",
            "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
            "{%- endif %}\n",
            "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
            "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
            "{%- if tools is not none and not tools_in_user_message %}\n",
            "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "{%- endif %}\n",
            "{{- system_message }}\n",
            "{{- \"<|eot_id|>\" }}\n",
            "\n",
            "{#- Custom tools are passed in a user message with some extra guidance #}\n",
            "{%- if tools_in_user_message and not tools is none %}\n",
            "    {#- Extract the first user message so we can plug it in here #}\n",
            "    {%- if messages | length != 0 %}\n",
            "        {%- set first_user_message = messages[0]['content']|trim %}\n",
            "        {%- set messages = messages[1:] %}\n",
            "    {%- else %}\n",
            "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
            "{%- endif %}\n",
            "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
            "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
            "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "    {{- first_user_message + \"<|eot_id|>\"}}\n",
            "{%- endif %}\n",
            "\n",
            "{%- for message in messages %}\n",
            "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
            "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
            "    {%- elif 'tool_calls' in message %}\n",
            "        {%- if not message.tool_calls|length == 1 %}\n",
            "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
            "        {%- endif %}\n",
            "        {%- set tool_call = message.tool_calls[0].function %}\n",
            "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
            "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
            "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
            "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
            "                {%- if not loop.last %}\n",
            "                    {{- \", \" }}\n",
            "                {%- endif %}\n",
            "                {%- endfor %}\n",
            "            {{- \")\" }}\n",
            "        {%- else  %}\n",
            "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
            "            {{- '\"parameters\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- \"}\" }}\n",
            "        {%- endif %}\n",
            "        {%- if builtin_tools is defined %}\n",
            "            {#- This means we're in ipython mode #}\n",
            "            {{- \"<|eom_id|>\" }}\n",
            "        {%- else %}\n",
            "            {{- \"<|eot_id|>\" }}\n",
            "        {%- endif %}\n",
            "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
            "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
            "        {%- if message.content is mapping or message.content is iterable %}\n",
            "            {{- message.content | tojson }}\n",
            "        {%- else %}\n",
            "            {{- message.content }}\n",
            "        {%- endif %}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "Using chat eos_token: <|eot_id|>\n",
            "Using chat bos_token: <|begin_of_text|>\n",
            "llama_perf_context_print:        load time =     185.16 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     185.39 ms /     7 tokens\n",
            "llama_perf_context_print:        load time =     185.16 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    45 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     117.38 ms /    46 tokens\n",
            "llama_perf_context_print:        load time =     185.16 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    40 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     117.33 ms /    41 tokens\n",
            "llama_perf_context_print:        load time =     185.16 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     116.96 ms /    36 tokens\n",
            "llama_perf_context_print:        load time =     185.16 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    49 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     117.35 ms /    50 tokens\n"
          ]
        }
      ],
      "source": [
        "# from langchain tutorial\n",
        "\n",
        "\"\"\"\n",
        "embedded_query = llama_embeddings.embed_query(query)\n",
        "embedded_docs = llama_embeddings.embed_documents(docs)\n",
        "\n",
        "print(f\"Embedding Dimension Output (Query): {len(embedded_query)}\")\n",
        "print(f\"Embedding Dimension Output (Docs): {len(embedded_docs[0])}\")\n",
        "\"\"\"\n",
        "\n",
        "# Overridden version of the LlamaCppEmbeddings class\n",
        "from typing import List\n",
        "from langchain_community.llms.llamacpp import LlamaCpp\n",
        "from langchain_community.embeddings.llamacpp import LlamaCppEmbeddings\n",
        "\n",
        "\n",
        "class CustomLlamaCppEmbeddings(LlamaCppEmbeddings):\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Embed a list of documents using the Llama model.\n",
        "\n",
        "        Args:\n",
        "            texts: The list of texts to embed.\n",
        "\n",
        "        Returns:\n",
        "            List of embeddings, one for each text.\n",
        "        \"\"\"\n",
        "        embeddings = [self.client.embed(text)[0] for text in texts]\n",
        "        return [list(map(float, e)) for e in embeddings]\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"Embed a query using the Llama model.\n",
        "\n",
        "        Args:\n",
        "            text: The text to embed.\n",
        "\n",
        "        Returns:\n",
        "            Embeddings for the text.\n",
        "        \"\"\"\n",
        "        embedding = self.client.embed(text)[0]\n",
        "        return list(map(float, embedding))\n",
        "\n",
        "\n",
        "c_embedder = CustomLlamaCppEmbeddings(model_path=model_path, n_gpu_layers=-1)\n",
        "embedded_query = c_embedder.embed_query(query)\n",
        "embedded_docs = c_embedder.embed_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check custom embeddings\n",
        "\n",
        "- To check whether the embedding results are output as expected, I output the dimensions of each embedding vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution_count": 11
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query embedding dimension: 4096\n",
            "Document embedding dimension: 4096\n"
          ]
        }
      ],
      "source": [
        "print(\"Query embedding dimension:\", len(embedded_query))\n",
        "print(\"Document embedding dimension:\", len(embedded_docs[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The similarity calculation results\n",
        "\n",
        "We can use the vector representations of the query and documents to calculate similarity.\n",
        "Here, we use the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) provided by scikit-learn.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution_count": 9
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.99999991 0.99999991 0.99999991 0.99999991]\n",
            "[Query] Langchain is awesome!\n",
            "====================================\n",
            "[0] similarity: 1.000 | In astronomy, the Drake Equation is a probabilistic argument used to estimate the number of active, communicative extraterrestrial civilizations in the Milky Way galaxy. It takes into account factors such as star formation rate and fraction of habitable planets.\n",
            "\n",
            "[1] similarity: 1.000 | C++ is a high-performance programming language widely used in system/software development, game programming, and real-time simulations. It supports both procedural and object-oriented paradigms.\n",
            "\n",
            "[2] similarity: 1.000 | The tropical island of Bali offers stunning beaches, volcanic mountains, lush forests, and vibrant coral reefs. Travelers often visit for surfing, yoga retreats, and the unique Balinese Hindu culture.\n",
            "\n",
            "[3] similarity: 1.000 | Spaghetti Carbonara is a traditional Italian pasta dish made with eggs, cheese, pancetta, and pepper. It's simple yet incredibly delicious. Typically served with spaghetti, but can also be enjoyed with other pasta types.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Calculate Cosine Similarity\n",
        "similarities = cosine_similarity([embedded_query], embedded_docs)[0]\n",
        "print(similarities)\n",
        "\n",
        "# Sort indices in ascending order.\n",
        "sorted_indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "print(f\"[Query] {query}\\n====================================\")\n",
        "for i, idx in enumerate(sorted_indices):\n",
        "    print(f\"[{i}] similarity: {similarities[idx]:.3f} | {docs[idx]}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "- [Llama-cpp Python GitHub](https://github.com/abetlen/llama-cpp-python)\n",
        "- [LangChain Documentation](https://langchain.readthedocs.io/en/latest/)\n",
        "- [Cosine Similarity - Wikipedia](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
        "----\n",
        "This concludes the **Llama-cpp Embeddings With Langchain** tutorial in the style of the original reference notebook."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-tutorial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
