{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weaviate\n",
    "\n",
    "- Author: [Haseom Shin](https://github.com/IHAGI-c)\n",
    "- Design: []()\n",
    "- Peer Review: []()\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/13-LangChain-Expression-Language/11-Fallbacks.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/13-LangChain-Expression-Language/11-Fallbacks.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers how to get started with the Weaviate vector store in LangChain, using the `langchain-weaviate` package.\n",
    "\n",
    "> [Weaviate](https://weaviate.io/) is an open-source vector database. It allows you to store data objects and vector embeddings from your favorite ML-models, and scale seamlessly into billions of data objects.\n",
    "\n",
    "To use this integration, you need to have a running Weaviate database instance.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "\n",
    "### References\n",
    "- [Langchain-Weaviate](https://python.langchain.com/docs/integrations/providers/weaviate/)\n",
    "- [Weaviate Documentation](https://weaviate.io/developers/weaviate)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"openai\",\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"tiktoken\",\n",
    "        \"langchain-weaviate\",\n",
    "        \"langchain-openai\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"WEAVIATE_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"Weaviate\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can alternatively set `OPENAI_API_KEY` in `.env` file and load it. \n",
    "\n",
    "[Note] This is not necessary if you've already set `OPENAI_API_KEY` in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Weaviate?\n",
    "\n",
    "Weaviate is a powerful open-source vector database that revolutionizes how we store and search data. It combines traditional database capabilities with advanced machine learning features, allowing you to:\n",
    "\n",
    "- Store both JSON documents and their vector embeddings in a unified system\n",
    "- Perform lightning-fast semantic searches across billions of data objects\n",
    "- Utilize built-in machine learning modules or bring your own vectors\n",
    "- Access data through an intuitive GraphQL API\n",
    "\n",
    "> 💡 **Key Feature**: Weaviate achieves millisecond-level query performance, making it suitable for production environments.\n",
    "\n",
    "## Why Use Weaviate?\n",
    "\n",
    "Weaviate stands out for several reasons:\n",
    "\n",
    "1. **Versatility**: Supports multiple media types (text, images, etc.)\n",
    "2. **Advanced Features**:\n",
    "   - Semantic Search\n",
    "   - Question-Answer Extraction\n",
    "   - Classification\n",
    "   - Custom ML Model Integration\n",
    "3. **Production-Ready**: Built in Go for high performance and scalability\n",
    "4. **Developer-Friendly**: Multiple access methods through GraphQL, REST, and various client libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Weaviate\n",
    "\n",
    "There are three main ways to connect to Weaviate:\n",
    "\n",
    "1. **Local Connection**: Connect to a Weaviate instance running locally through Docker\n",
    "2. **Weaviate Cloud Services (WCS)**: Use Weaviate's managed cloud service\n",
    "3. **Custom Deployment**: Deploy Weaviate on Kubernetes or other custom configurations\n",
    "\n",
    "For this notebook, we'll use Weaviate Cloud Services (WCS) as it provides the easiest way to get started without any local setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Weaviate Cloud Services\n",
    "\n",
    "1. First, sign up for a free account at [Weaviate Cloud Console](https://console.weaviate.cloud)\n",
    "2. Create a new cluster and get your API key\n",
    "3. Connect to your WCS cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate import connect_to_wcs\n",
    "from weaviate.auth import AuthApiKey\n",
    "import os\n",
    "# Connect to the Weaviate instance\n",
    "weaviate_client = connect_to_wcs(\n",
    "    auth_credentials=AuthApiKey(api_key=os.getenv(\"WEAVIATE_API_KEY\")),\n",
    "    cluster_url=\"https://6s4qfg5urvg10hhctpsktg.c0.us-west3.gcp.weaviate.cloud\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Objects by Similarity\n",
    "\n",
    "Weaviate allows you to find objects that are semantically similar to your query. Let's walk through a complete example, from importing data to executing similarity searches.\n",
    "\n",
    "### Step 1: Preparing Your Data\n",
    "\n",
    "Before we can perform similarity searches, we need to populate our Weaviate instance with data. We'll start by loading and chunking a text file into manageable pieces.\n",
    "\n",
    "> 💡 **Tip**: Breaking down large texts into smaller chunks helps optimize vector search performance and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Create a document with metadata, including geo-information\n",
    "raw_texts = [\n",
    "    \"The Eiffel Tower in Paris stands 324 meters tall and was completed in 1889.\",\n",
    "    \"The Great Wall of China is over 21,000 kilometers long and was built over several centuries.\",\n",
    "    \"The Taj Mahal in India was built by Emperor Shah Jahan as a tomb for his beloved wife.\",\n",
    "    \"Machu Picchu in Peru was built by the Inca Empire in the 15th century at an altitude of 2,430 meters.\",\n",
    "    \"The Pyramids of Giza in Egypt were built over 4,500 years ago as tombs for pharaohs.\",\n",
    "    \"The Colosseum in Rome could hold up to 50,000 spectators for gladiatorial contests.\",\n",
    "    \"Petra in Jordan was carved into rose-colored rock faces and served as a trading center.\",\n",
    "    \"Angkor Wat in Cambodia is the world's largest religious monument, built in the 12th century.\"\n",
    "]\n",
    "\n",
    "# 각 텍스트에 해당하는 지역 정보\n",
    "regions = [\n",
    "    \"Europe\",    # Eiffel Tower\n",
    "    \"Asia\",      # Great Wall\n",
    "    \"Asia\",      # Taj Mahal\n",
    "    \"South America\",  # Machu Picchu\n",
    "    \"Africa\",    # Pyramids\n",
    "    \"Europe\",    # Colosseum\n",
    "    \"Asia\",      # Petra\n",
    "    \"Asia\"       # Angkor Wat\n",
    "]\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=text, metadata={\"region\": region}) \n",
    "    for text, region in zip(raw_texts, regions)\n",
    "]\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "db = WeaviateVectorStore.from_documents(docs, embeddings, client=weaviate_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Perform the search\n",
    "\n",
    "We can now perform a similarity search. This will return the most similar documents to the query text, based on the embeddings stored in Weaviate and an equivalent embedding generated from the query text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1:\n",
      "Petra in Jordan was carved into rose-colored rock faces and served as a trading center.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Petra?\"\n",
    "docs = db.similarity_search(query, k=1)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add filters, which will either include or exclude results based on the filter conditions. (See [more filter examples](https://weaviate.io/developers/weaviate/search/filters).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Monuments in Europe ===\n",
      "Found 2 results:\n",
      "\n",
      "Document 1:\n",
      "Content: The Colosseum in Rome could hold up to 50,000 spectators for gladiatorial contests.\n",
      "Region: Europe\n",
      "\n",
      "Document 2:\n",
      "Content: The Eiffel Tower in Paris stands 324 meters tall and was completed in 1889.\n",
      "Region: Europe\n",
      "\n",
      "=== Monuments in Asia ===\n",
      "Found 4 results:\n",
      "\n",
      "Document 1:\n",
      "Content: Petra in Jordan was carved into rose-colored rock faces and served as a trading center.\n",
      "Region: Asia\n",
      "\n",
      "Document 2:\n",
      "Content: Angkor Wat in Cambodia is the world's largest religious monument, built in the 12th century.\n",
      "Region: Asia\n",
      "\n",
      "Document 3:\n",
      "Content: The Taj Mahal in India was built by Emperor Shah Jahan as a tomb for his beloved wife.\n",
      "Region: Asia\n",
      "\n",
      "Document 4:\n",
      "Content: The Great Wall of China is over 21,000 kilometers long and was built over several centuries.\n",
      "Region: Asia\n",
      "\n",
      "=== Monuments in Asia ===\n",
      "Found 4 results:\n",
      "\n",
      "Document 1:\n",
      "Content: Petra in Jordan was carved into rose-colored rock faces and served as a trading center.\n",
      "Region: Asia\n",
      "\n",
      "Document 2:\n",
      "Content: Angkor Wat in Cambodia is the world's largest religious monument, built in the 12th century.\n",
      "Region: Asia\n",
      "\n",
      "Document 3:\n",
      "Content: The Taj Mahal in India was built by Emperor Shah Jahan as a tomb for his beloved wife.\n",
      "Region: Asia\n",
      "\n",
      "Document 4:\n",
      "Content: The Great Wall of China is over 21,000 kilometers long and was built over several centuries.\n",
      "Region: Asia\n",
      "\n",
      "=== Monuments in South America ===\n",
      "Found 1 results:\n",
      "\n",
      "Document 1:\n",
      "Content: Machu Picchu in Peru was built by the Inca Empire in the 15th century at an altitude of 2,430 meters.\n",
      "Region: South America\n",
      "\n",
      "=== Monuments in Africa ===\n",
      "Found 1 results:\n",
      "\n",
      "Document 1:\n",
      "Content: The Pyramids of Giza in Egypt were built over 4,500 years ago as tombs for pharaohs.\n",
      "Region: Africa\n",
      "\n",
      "=== Monuments in Europe ===\n",
      "Found 2 results:\n",
      "\n",
      "Document 1:\n",
      "Content: The Colosseum in Rome could hold up to 50,000 spectators for gladiatorial contests.\n",
      "Region: Europe\n",
      "\n",
      "Document 2:\n",
      "Content: The Eiffel Tower in Paris stands 324 meters tall and was completed in 1889.\n",
      "Region: Europe\n",
      "\n",
      "=== Monuments in Asia ===\n",
      "Found 4 results:\n",
      "\n",
      "Document 1:\n",
      "Content: Petra in Jordan was carved into rose-colored rock faces and served as a trading center.\n",
      "Region: Asia\n",
      "\n",
      "Document 2:\n",
      "Content: Angkor Wat in Cambodia is the world's largest religious monument, built in the 12th century.\n",
      "Region: Asia\n",
      "\n",
      "Document 3:\n",
      "Content: The Taj Mahal in India was built by Emperor Shah Jahan as a tomb for his beloved wife.\n",
      "Region: Asia\n",
      "\n",
      "Document 4:\n",
      "Content: The Great Wall of China is over 21,000 kilometers long and was built over several centuries.\n",
      "Region: Asia\n",
      "\n",
      "=== Monuments in Asia ===\n",
      "Found 4 results:\n",
      "\n",
      "Document 1:\n",
      "Content: Petra in Jordan was carved into rose-colored rock faces and served as a trading center.\n",
      "Region: Asia\n",
      "\n",
      "Document 2:\n",
      "Content: Angkor Wat in Cambodia is the world's largest religious monument, built in the 12th century.\n",
      "Region: Asia\n",
      "\n",
      "Document 3:\n",
      "Content: The Taj Mahal in India was built by Emperor Shah Jahan as a tomb for his beloved wife.\n",
      "Region: Asia\n",
      "\n",
      "Document 4:\n",
      "Content: The Great Wall of China is over 21,000 kilometers long and was built over several centuries.\n",
      "Region: Asia\n"
     ]
    }
   ],
   "source": [
    "from weaviate.classes.query import Filter\n",
    "\n",
    "for region in regions:\n",
    "    search_filter = Filter.by_property(\"region\").equal(region)\n",
    "    filtered_results = db.similarity_search(query, filters=search_filter, k=4)\n",
    "    \n",
    "    print(f\"\\n=== Monuments in {region} ===\")\n",
    "    print(f\"Found {len(filtered_results)} results:\")\n",
    "    for i, doc in enumerate(filtered_results, 1):\n",
    "        print(f\"\\nDocument {i}:\")\n",
    "        print(f\"Content: {doc.page_content}\")\n",
    "        print(f\"Region: {doc.metadata['region']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to provide `k`, which is the upper limit of the number of results to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Limiting Results with k parameter ===\n",
      "\n",
      "Searching for monuments in Europe with k=3:\n",
      "Number of results: 2\n",
      "\n",
      "Result 1:\n",
      "Content: The Colosseum in Rome could hold up to 50,000 spectators for gladiatorial contests.\n",
      "\n",
      "Result 2:\n",
      "Content: The Eiffel Tower in Paris stands 324 meters tall and was completed in 1889.\n",
      "\n",
      "Verification: ✓ Number of results is correctly limited by k parameter\n"
     ]
    }
   ],
   "source": [
    "# Using the k parameter to limit the number of results\n",
    "print(\"\\n=== Limiting Results with k parameter ===\")\n",
    "search_filter = Filter.by_property(\"region\").equal(regions[0])  # Europe\n",
    "filtered_search_results = db.similarity_search(query, filters=search_filter, k=3)\n",
    "print(f\"\\nSearching for monuments in {regions[0]} with k=3:\")\n",
    "print(f\"Number of results: {len(filtered_search_results)}\")\n",
    "for i, doc in enumerate(filtered_search_results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "\n",
    "# Check if the number of results is k or less\n",
    "assert len(filtered_search_results) <= 3, f\"Expected 3 or fewer results, but got {len(filtered_search_results)}\"\n",
    "print(\"\\nVerification: ✓ Number of results is correctly limited by k parameter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantify Result Similarity\n",
    "\n",
    "When performing similarity searches, you might want to know not just which documents are similar, but how similar they are. Weaviate provides this information through a relevance score.\n",
    "> 💡 Tip: The relevance score helps you understand the relative similarity between search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 : Angkor Wat in Cambodia is the world's largest religious monument, built in the 12th century.\n",
      "0.729 : The Taj Mahal in India was built by Emperor Shah Jahan as a tomb for his beloved wife.\n",
      "0.527 : Petra in Jordan was carved into rose-colored rock faces and served as a trading center.\n",
      "0.510 : The Great Wall of China is over 21,000 kilometers long and was built over several centuries.\n",
      "0.304 : The Pyramids of Giza in Egypt were built over 4,500 years ago as tombs for pharaohs.\n"
     ]
    }
   ],
   "source": [
    "docs = db.similarity_search_with_score(\"What monuments are in Asia?\", k=5)\n",
    "\n",
    "for doc in docs:\n",
    "    print(f\"{doc[1]:.3f}\", \":\", doc[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`similarity_search` uses Weaviate's [hybrid search](https://weaviate.io/developers/weaviate/api/graphql/search-operators#hybrid).\n",
    "\n",
    "A hybrid search combines a vector and a keyword search, with `alpha` as the weight of the vector search. The `similarity_search` function allows you to pass additional arguments as kwargs. See this [reference doc](https://weaviate.io/developers/weaviate/api/graphql/search-operators#hybrid) for the available arguments.\n",
    "\n",
    "So, you can perform a pure keyword search by adding `alpha=0` as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'region': 'Asia'}, page_content='Petra in Jordan was carved into rose-colored rock faces and served as a trading center.')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = db.similarity_search(query, alpha=0)\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any data added through `langchain-weaviate` will persist in Weaviate according to its configuration. \n",
    "\n",
    "WCS instances, for example, are configured to persist data indefinitely, and Docker instances can be set up to persist data in a volume. Read more about [Weaviate's persistence](https://weaviate.io/developers/weaviate/configuration/persistence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-tenancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Multi-tenancy](https://weaviate.io/developers/weaviate/concepts/data#multi-tenancy) allows you to have a high number of isolated collections of data, with the same collection configuration, in a single Weaviate instance. This is great for multi-user environments such as building a SaaS app, where each end user will have their own isolated data collection.\n",
    "\n",
    "To use multi-tenancy, the vector store need to be aware of the `tenant` parameter. \n",
    "\n",
    "So when adding any data, provide the `tenant` parameter as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-Jan-07 09:20 PM - langchain_weaviate.vectorstores - INFO - Tenant Foo does not exist in index LangChain_36336a662af7405b89d8d1e60ab90c5c. Creating tenant.\n"
     ]
    }
   ],
   "source": [
    "db_with_mt = WeaviateVectorStore.from_documents(\n",
    "    docs, embeddings, client=weaviate_client, tenant=\"Foo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when performing queries, provide the `tenant` parameter also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'region': 'Asia'}, page_content='Petra in Jordan was carved into rose-colored rock faces and served as a trading center.')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_with_mt.similarity_search(query, tenant=\"Foo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever options\n",
    "\n",
    "Weaviate can also be used as a retriever\n",
    "\n",
    "### Maximal marginal relevance search (MMR)\n",
    "\n",
    "In addition to using similaritysearch  in the retriever object, you can also use `mmr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'region': 'Asia'}, page_content='Petra in Jordan was carved into rose-colored rock faces and served as a trading center.')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever(search_type=\"mmr\")\n",
    "retriever.invoke(query)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A known limitation of large language models (LLMs) is that their training data can be outdated, or not include the specific domain knowledge that you require.\n",
    "\n",
    "Take a look at the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Eiffel Tower is a famous landmark in Paris, France. It is a wrought iron lattice tower that was built for the 1889 World's Fair and has since become a global symbol of France and one of the most recognizable structures in the world. It stands at 1,063 feet tall and is one of the most visited tourist attractions in the world.\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm.predict(\"What is Eiffel Tower?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector stores complement LLMs by providing a way to store and retrieve relevant information. This allow you to combine the strengths of LLMs and vector stores, by using LLM's reasoning and linguistic capabilities with vector stores' ability to retrieve relevant information.\n",
    "\n",
    "Two well-known applications for combining LLMs and vector stores are:\n",
    "- Question answering\n",
    "- Retrieval-augmented generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering with Sources\n",
    "\n",
    "Question answering in langchain can be enhanced by the use of vector stores. Let's see how this can be done.\n",
    "\n",
    "This section uses the `RetrievalQAWithSourcesChain`, which does the lookup of the documents from an Index. \n",
    "\n",
    "First, we will chunk the text again and import them into the Weaviate vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = WeaviateVectorStore.from_texts(\n",
    "    raw_texts,\n",
    "    embeddings,\n",
    "    client=weaviate_client,\n",
    "    metadatas=[{\"source\": f\"{i}-pl\"} for i in range(len(raw_texts))],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct the chain, with the retriever specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    OpenAI(temperature=0), chain_type=\"stuff\", retriever=docsearch.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': ' The Eiffel Tower is a 324-meter tall structure located in Paris, France, completed in 1889.\\n',\n",
       " 'sources': '0-pl'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\n",
    "    {\"question\": \"What is Eiffel Tower?\"},\n",
    "    return_only_outputs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation\n",
    "\n",
    "Another very popular application of combining LLMs and vector stores is retrieval-augmented generation (RAG). This is a technique that uses a retriever to find relevant information from a vector store, and then uses an LLM to provide an output based on the retrieved data and a prompt.\n",
    "\n",
    "We begin with a similar setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"state_of_the_union.txt\") as f:\n",
    "#     state_of_the_union = f.read()\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "# texts = text_splitter.split_text(state_of_the_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = WeaviateVectorStore.from_texts(\n",
    "    raw_texts,\n",
    "    embeddings,\n",
    "    client=weaviate_client,\n",
    "    metadatas=[{\"source\": f\"{i}-pl\"} for i in range(len(raw_texts))],\n",
    ")\n",
    "\n",
    "retriever = docsearch.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to construct a template for the RAG model so that the retrieved information will be populated in the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Petra is an archaeological site in Jordan, known for its stunning architecture carved into rose-colored rock faces. It served as a significant trading center in ancient times.'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Petra?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-BXw0bE1H-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
